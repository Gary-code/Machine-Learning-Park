### 1 Linear Regression

> çº¿æ€§å›å½’
>
> æœ¬æ–‡Githubä»“åº“å·²ç»åŒæ­¥æ–‡ç« ä¸ä»£ç [https://github.com/Gary-code/Machine-Learning-Park/tree/main/1%20LinearRegression](https://github.com/Gary-code/Machine-Learning-Park/tree/main/1%20LinearRegression)

ä»£ç è¯´æ˜ï¼š

| æ–‡ä»¶å          | è¯´æ˜                                    |
| --------------- | --------------------------------------- |
| numpy_version   | numpyå®ç°ä½¿ç”¨data.txtæ•°æ®é›†             |
| pytorch_version | åŒ…æ‹¬ä½¿ç”¨pytorchä»é›¶å¼€å§‹å®ç°å’Œè°ƒç”¨åŒ…å®ç° |





#### 1.1 Linear Algebra

> çº¿æ€§ä»£æ•°

##### çŸ©é˜µä¸å‘é‡

$$
X = \left[\begin{array}{cc}
1402 & 191 \\
1371 & 821 \\
949 & 1437 \\
147 & 1448
\end{array}\right]
$$



> ä¸€èˆ¬é»˜è®¤éƒ½ä»¥åˆ—å‘é‡ä¸ºç ”ç©¶å¯¹è±¡
>
> * ä¸º $n \times 1$çš„çŸ©é˜µ

$$
y=\left[\begin{array}{l}
460 \\
232 \\
315 \\
178
\end{array}\right]
$$



##### è¿ç®—

* çŸ©é˜µä¹˜æ³•

![](https://raw.githubusercontent.com/Gary-code/Machine-Learning-Park/files/blogImgs/20211119111147.png)

* çŸ©é˜µä¹˜æ³•çš„ç‰¹å¾

  * ä¸æ»¡è¶³äº¤æ¢å¾‹
  * æ»¡è¶³ç»“åˆå¾‹

  $$
  A \times B \neq B \times A  \\
  A \times B \times C = A \times (B \times C)
  $$

  

* å•ä½çŸ©é˜µï¼š Identity Matrix

$$
I_{4 \times 4} = \left[\begin{array}{llll}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]
$$


$$
AA^{-1} = A{-1}A = I \\
AI = IA = A
$$

##### é€†ä¸è½¬ç½®

* çŸ©é˜µçš„é€†ï¼š

  * å¦‚çŸ©é˜µğ´æ˜¯ä¸€ä¸ªğ‘š Ã— ğ‘šçŸ©é˜µï¼ˆæ–¹é˜µï¼‰ï¼Œå¦‚æœæœ‰é€†çŸ©é˜µï¼Œåˆ™ï¼š$ ğ´ğ´^{-1}$ = $ğ´^{âˆ’1}ğ´$ = ğ¼

* çŸ©é˜µçš„è½¬ç½®ï¼š

  * $$
    \left|\begin{array}{ll}
    a & b \\
    c & d \\
    e & f
    \end{array}\right|^{T}=\left|\begin{array}{lll}
    a & c & e \\
    b & d & f
    \end{array}\right|
    $$

  * åŸºæœ¬æ€§è´¨
    $$
    \begin{aligned}
    &(A \pm B)^{T}=A^{T} \pm B^{T} \\
    &(A \times B)^{T}=B^{T} \times A^{T} \\
    &\left(A^{T}\right)^{T}=A \\
    &(K A)^{T}=K A^{T}
    \end{aligned}
    $$
    

#### 1.2 çº¿æ€§å›å½’æ¨¡å‹

##### è¡¨è¾¾å¼

$$
Y = WX + b
$$

$w$ä¸ºå˜é‡$X$çš„ç³»æ•°ï¼Œ$b$ä¸ºåç½®é¡¹ã€‚

##### æŸå¤±å‡½æ•°

> Loss Function -- MSE

$$
J =\frac{1}{2m}\sum_{i=1}^{m}(y^i - y)^2
$$

$m$ä¸ºæ ·æœ¬çš„ä¸ªæ•°

##### æ¨¡å‹

Hypothesis:      $\quad h(X)=b+WX$

Parameters:         $W, b$

Cost Function: $\quad J=\frac{1}{2 m} \sum_{i=1}^{m}\left(h\left(x^{(i)}\right)-y^{(i)}\right)^{2}$

Goal:            	$\quad \operatorname{minimize}_{b, W} J$



##### æ±‚è§£

* æ¢¯åº¦ä¸‹é™ç®—æ³•

![](https://raw.githubusercontent.com/Gary-code/Machine-Learning-Park/files/blogImgs/image-20210714205526599.png)

åœ¨æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­ï¼Œè¿˜æœ‰ä¸€ä¸ªæ›´å¾®å¦™çš„é—®é¢˜ï¼Œæ¢¯åº¦ä¸‹é™ä¸­ï¼Œæˆ‘ä»¬è¦æ›´æ–°$ğœƒ_0$å’Œ$ğœƒ_1$ ï¼Œå½“ ğ‘— = 0 å’Œğ‘— = 1æ—¶ï¼Œä¼šäº§ç”Ÿæ›´æ–°ï¼Œæ‰€ä»¥ä½ å°†æ›´æ–°$ğ½(ğœƒ_0)$å’Œ$ğ½(ğœƒ_1)$ã€‚å®ç°æ¢¯åº¦ä¸‹é™ç®—æ³•çš„å¾®å¦™ä¹‹å¤„æ˜¯ï¼Œåœ¨è¿™ä¸ªè¡¨è¾¾å¼ä¸­ï¼Œå¦‚æœä½ è¦æ›´æ–°è¿™ä¸ªç­‰å¼ï¼Œä½ éœ€è¦åŒæ—¶æ›´æ–°$ğœƒ_0$å’Œ$ğœƒ_1$ï¼Œæˆ‘çš„æ„æ€æ˜¯åœ¨è¿™ä¸ªç­‰å¼ä¸­ï¼Œæˆ‘ä»¬è¦è¿™æ ·æ›´æ–°ï¼š$ğœƒ_0:= ğœƒ_0$ ï¼Œå¹¶æ›´æ–°$ğœƒ_1:= ğœƒ_1$ã€‚å®ç°æ–¹æ³•æ˜¯ï¼šä½ åº”è¯¥è®¡ç®—å…¬å¼å³è¾¹çš„éƒ¨åˆ†ï¼Œé€šè¿‡é‚£ä¸€éƒ¨åˆ†è®¡ç®—å‡º$ğœƒ_0$å’Œ$ğœƒ_1$çš„å€¼ï¼Œç„¶ååŒæ—¶æ›´æ–°$ğœƒ_0$å’Œ$ğœƒ_1$ã€‚

* $\alpha$æ˜¯å­¦ä¹ ç‡ï¼Œç”¨æ¥æ§åˆ¶ä¸‹é™ä½ è¦è¿ˆå‡ºå¤šå¤§çš„æ­¥å­
* å®Œæˆä¸€è½®ä¹‹åï¼Œæ›´æ–°ä¸¤ä¸ªç³»æ•°$ğœƒ_0:= ğœƒ_0$ ï¼Œ$ğœƒ_1:= ğœƒ_1$
  * **è®°ä½åšå®Œä¸€è½®æ‰æ›´æ–°ï¼Œä¸è¦ä¸€ä¸ªä¸€ä¸ªç³»æ•°è½®ç€æ¥æ›´æ–°**

* å¦‚æœğ‘å¤ªå°äº†ï¼Œå³æˆ‘çš„å­¦ä¹ é€Ÿç‡å¤ªå°ï¼Œç»“æœå°±æ˜¯åªèƒ½è¿™æ ·åƒå°å®å®ä¸€æ ·ä¸€ç‚¹ç‚¹åœ°æŒªåŠ¨ï¼Œå»åŠªåŠ›æ¥è¿‘æœ€ä½ç‚¹ï¼Œè¿™æ ·å°±éœ€è¦å¾ˆå¤šæ­¥æ‰èƒ½åˆ°è¾¾æœ€ä½ç‚¹ï¼Œæ‰€ä»¥å¦‚æœğ‘å¤ªå°çš„è¯ï¼Œå¯èƒ½ä¼šå¾ˆæ…¢ï¼Œå› ä¸ºå®ƒä¼šä¸€ç‚¹ç‚¹æŒªåŠ¨ï¼Œå®ƒä¼šéœ€è¦å¾ˆå¤šæ­¥æ‰èƒ½åˆ°è¾¾å…¨å±€æœ€ä½ç‚¹ã€‚

* å¦‚æœğ‘å¤ªå¤§ï¼Œé‚£ä¹ˆæ¢¯åº¦ä¸‹é™æ³•å¯èƒ½ä¼šè¶Šè¿‡æœ€ä½ç‚¹ï¼Œç”šè‡³å¯èƒ½æ— æ³•æ”¶æ•›ï¼Œä¸‹ä¸€æ¬¡è¿­ä»£åˆç§»åŠ¨äº†ä¸€å¤§æ­¥ï¼Œè¶Šè¿‡ä¸€æ¬¡ï¼Œåˆè¶Šè¿‡ä¸€æ¬¡ï¼Œä¸€æ¬¡æ¬¡è¶Šè¿‡æœ€ä½ç‚¹ï¼Œç›´åˆ°ä½ å‘ç°å®é™…ä¸Šç¦»æœ€ä½ç‚¹è¶Šæ¥

![](https://raw.githubusercontent.com/Gary-code/Machine-Learning-Park/files/blogImgs/image-20210714211745067.png)

* æ‰¹é‡æ¢¯åº¦ä¸‹é™
  * æŒ‡çš„æ˜¯åœ¨æ¢¯åº¦ä¸‹é™çš„æ¯ä¸€æ­¥ä¸­ï¼Œå®ƒæ˜¯æŒ‡åœ¨**æ¯ä¸€æ¬¡è¿­ä»£æ—¶**ä½¿ç”¨**æ‰€æœ‰æ ·æœ¬**æ¥è¿›è¡Œæ¢¯åº¦çš„æ›´æ–°ã€‚

$$
\theta_j := \theta_j - \alpha  \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}
$$

å¯¹æ¯”éšæœºæ¢¯åº¦ä¸‹é™

* **æ¯æ¬¡è¿­ä»£**ä½¿ç”¨**ä¸€ä¸ªæ ·æœ¬**æ¥å¯¹å‚æ•°è¿›è¡Œæ›´æ–°ã€‚ä½¿å¾—è®­ç»ƒé€Ÿåº¦åŠ å¿«ã€‚
* $repeat${
   â€ƒâ€ƒâ€ƒfor $i=1,...,m${

$$
\theta_j := \theta_j -\alpha (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}  (for j =0,1)
$$

â€‹	}
 â€ƒ	}

##### è¿›é˜¶

* å°ºåº¦ç¼©æ”¾

åœ¨æˆ‘ä»¬é¢å¯¹å¤šç»´ç‰¹å¾é—®é¢˜çš„æ—¶å€™ï¼Œ**æˆ‘ä»¬è¦ä¿è¯è¿™äº›ç‰¹å¾éƒ½å…·æœ‰ç›¸è¿‘çš„å°ºåº¦**ï¼ˆèŒƒå›´ç±»ä¼¼ï¼‰ï¼Œè¿™å°†å¸®åŠ©æ¢¯åº¦ä¸‹é™ç®—æ³•æ›´å¿«åœ°æ”¶æ•›ã€‚
$$
X_ğ‘› = \frac{X_n-\mu_n}{s_n}
$$
å…¶ä¸­ $ğœ‡_ğ‘›$æ˜¯å¹³å‡å€¼ï¼Œ$ğ‘ _ğ‘›$æ˜¯æ ‡å‡†å·®(å³$max - min$)ã€‚



* å­¦ä¹ ç‡ $\alpha$
  * é€šå¸¸å¯ä»¥è€ƒè™‘å°è¯•äº›å­¦ä¹ ç‡ï¼š$\alpha = 0.01, 0.03 ,0.1, 0.3 ,1, 3, 10$



* æ­£è§„æ–¹ç¨‹

> normal Equation

$$
ğœƒ = (ğ‘‹^ğ‘‡ğ‘‹)^{-1}ğ‘‹^ğ‘‡ğ‘¦
$$

ä½¿ç”¨pythonå®ç°å¦‚ä¸‹ï¼š

```python
import numpy as np
def normalEqn(X, y):
    theta = np.linalg.inv(X.T@X)@X.T@y #X.T@X ç­‰ä»·äº X.T.dot(X)
    return theta
```

**æ³¨æ„**ï¼ï¼ï¼š**å¯¹äºé‚£äº›ä¸å¯é€†çš„çŸ©é˜µ**ï¼Œæ­£è§„æ–¹ç¨‹æ–¹æ³•æ˜¯ä¸èƒ½ç”¨çš„ã€‚

åœ¨å¤§è§„æ¨¡çš„æ•°æ®æ—¶å€™**ä¼˜å…ˆä½¿ç”¨æ¢¯åº¦ä¸‹é™**è€Œéæ­£è§„æ–¹ç¨‹



* æ­£åˆ™é¡¹

$$
J = J_{åŸ} + \lambda \sum_{j=1}^{m}\theta_j^2\\
J_{åŸ} = \frac{1}{2 m} \sum_{i=1}^{m}\left(h\left(x^{(i)}\right)-y^{(i)}\right)^{2}
$$

ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

