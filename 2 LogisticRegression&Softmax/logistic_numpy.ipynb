{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 逻辑回归实现\n",
    "> numpy实现\n",
    "> 使用a9a.txt的是数据集做二分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import Accumulator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取数据集\n",
    "> 使用train_test_split函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "x_val, y_val = load_svmlight_file('./a9a.txt', n_features=123)\n",
    "x_train, y_train = load_svmlight_file('./a9a_train.txt', n_features=123)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据预处理\n",
    "观察可以看到分类为-1,1\n",
    "\n",
    "为了方便损失函数的计算我们把-1转变成0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "y_val[y_val == -1] = 0\n",
    "y_train[y_train == -1] = 0\n",
    "\n",
    "# 转换成为np\n",
    "x_train = np.array(x_train.todense())\n",
    "x_val = np.array(x_val.todense())\n",
    "y_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "y_val = np.array(y_val).reshape(len(y_val), 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 参数设置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 1., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 1., ..., 0., 0., 1.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 1.],\n       [1., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 1.]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 插入偏置量\n",
    "b = np.ones(x_train.shape[0])\n",
    "x_train = np.insert(x_train, 123, values=b, axis=1)\n",
    "\n",
    "b = np.ones(x_val.shape[0])\n",
    "x_val = np.insert(x_val, 123, values=b, axis=1)\n",
    "\n",
    "# 超参数\n",
    "theta = np.random.normal(size=(x_train.shape[1], 1))\n",
    "\n",
    "x_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义损失函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"sigmoid函数\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logisreg(x, w):\n",
    "    \"\"\"逻辑回归模型\"\"\"\n",
    "    return x @ w\n",
    "\n",
    "def cost(model, y):\n",
    "    \"\"\"损失函数（交叉熵）\"\"\"\n",
    "    model = model.reshape(y.shape)\n",
    "    first = -y.T @ np.log(sigmoid(model))\n",
    "    second = (1 - y).T @ np.log(1 - sigmoid(model))\n",
    "    return (first - second) / len(y)\n",
    "\n",
    "def step_gradient(w_cur, x_value, y_value, lr):\n",
    "    \"\"\"\n",
    "    梯度下降\n",
    "    :param w_cur:\n",
    "    :param x_value:\n",
    "    :param y_value:\n",
    "    :param lr: 学习率\n",
    "    :return: 更新后的w参数\n",
    "    \"\"\"\n",
    "    grad = 1 / len(y_value) * x_value.T @ (sigmoid(x_value @ w_cur) - y_value)\n",
    "    new_w = w_cur - lr * grad\n",
    "    return new_w\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 计算准确性函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 预测检验\n",
    "def accuracy(y_hat, y, threshold=0.7):  # 阈值设置为0.7\n",
    "    \"\"\"计算预测正确数量\"\"\"\n",
    "    if len(y_hat.shape) > 1:\n",
    "        y_hat = sigmoid(y_hat)\n",
    "    y_hat[y_hat <= 0.7] = 0.\n",
    "    y_hat[y_hat > 0.7] = 1.\n",
    "    cmp = y_hat == y\n",
    "    return float(cmp.sum())\n",
    "\n",
    "\n",
    "# 评估任意模型net的准确率\n",
    "def evaluate_accuracy(model, y):\n",
    "    \"\"\"计算在指定数据集上模型的精度\"\"\"\n",
    "    metric = Accumulator(2)  # 正确预测数, 预测总数\n",
    "    metric.add(accuracy(model, y), len(y))\n",
    "    return metric[0] / metric[1]\n",
    "\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)  # 样本数\n",
    "    # 这些样本是随机读取的\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = np.array(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss_train: 0.865168\n",
      "epoch 1, loss_val: 0.836950\n",
      "epoch 1, accuracy: 0.712118\n",
      "=================================================================================\n",
      "epoch 2, loss_train: 0.690736\n",
      "epoch 2, loss_val: 0.705450\n",
      "epoch 2, accuracy: 0.751305\n",
      "=================================================================================\n",
      "epoch 3, loss_train: 0.722224\n",
      "epoch 3, loss_val: 0.664761\n",
      "epoch 3, accuracy: 0.764941\n",
      "=================================================================================\n",
      "epoch 4, loss_train: 0.637168\n",
      "epoch 4, loss_val: 0.643748\n",
      "epoch 4, accuracy: 0.771451\n",
      "=================================================================================\n",
      "epoch 5, loss_train: 0.654321\n",
      "epoch 5, loss_val: 0.628170\n",
      "epoch 5, accuracy: 0.773601\n",
      "=================================================================================\n",
      "epoch 6, loss_train: 0.676959\n",
      "epoch 6, loss_val: 0.614944\n",
      "epoch 6, accuracy: 0.775874\n",
      "=================================================================================\n",
      "epoch 7, loss_train: 0.700486\n",
      "epoch 7, loss_val: 0.603037\n",
      "epoch 7, accuracy: 0.779805\n",
      "=================================================================================\n",
      "epoch 8, loss_train: 0.598529\n",
      "epoch 8, loss_val: 0.592172\n",
      "epoch 8, accuracy: 0.782691\n",
      "=================================================================================\n",
      "epoch 9, loss_train: 0.550900\n",
      "epoch 9, loss_val: 0.582186\n",
      "epoch 9, accuracy: 0.785271\n",
      "=================================================================================\n",
      "epoch 10, loss_train: 0.550091\n",
      "epoch 10, loss_val: 0.572988\n",
      "epoch 10, accuracy: 0.786745\n",
      "=================================================================================\n",
      "epoch 11, loss_train: 0.525847\n",
      "epoch 11, loss_val: 0.564464\n",
      "epoch 11, accuracy: 0.788342\n",
      "=================================================================================\n",
      "epoch 12, loss_train: 0.587021\n",
      "epoch 12, loss_val: 0.556488\n",
      "epoch 12, accuracy: 0.789202\n",
      "=================================================================================\n",
      "epoch 13, loss_train: 0.520760\n",
      "epoch 13, loss_val: 0.549059\n",
      "epoch 13, accuracy: 0.790615\n",
      "=================================================================================\n",
      "epoch 14, loss_train: 0.515057\n",
      "epoch 14, loss_val: 0.542076\n",
      "epoch 14, accuracy: 0.792396\n",
      "=================================================================================\n",
      "epoch 15, loss_train: 0.550508\n",
      "epoch 15, loss_val: 0.535527\n",
      "epoch 15, accuracy: 0.793195\n",
      "=================================================================================\n",
      "epoch 16, loss_train: 0.480422\n",
      "epoch 16, loss_val: 0.529321\n",
      "epoch 16, accuracy: 0.794791\n",
      "=================================================================================\n",
      "epoch 17, loss_train: 0.476798\n",
      "epoch 17, loss_val: 0.523467\n",
      "epoch 17, accuracy: 0.796020\n",
      "=================================================================================\n",
      "epoch 18, loss_train: 0.494274\n",
      "epoch 18, loss_val: 0.517896\n",
      "epoch 18, accuracy: 0.797863\n",
      "=================================================================================\n",
      "epoch 19, loss_train: 0.476496\n",
      "epoch 19, loss_val: 0.512625\n",
      "epoch 19, accuracy: 0.798722\n",
      "=================================================================================\n",
      "epoch 20, loss_train: 0.471943\n",
      "epoch 20, loss_val: 0.507603\n",
      "epoch 20, accuracy: 0.799459\n",
      "=================================================================================\n",
      "epoch 21, loss_train: 0.531281\n",
      "epoch 21, loss_val: 0.502804\n",
      "epoch 21, accuracy: 0.800442\n",
      "=================================================================================\n",
      "epoch 22, loss_train: 0.459699\n",
      "epoch 22, loss_val: 0.498227\n",
      "epoch 22, accuracy: 0.801179\n",
      "=================================================================================\n",
      "epoch 23, loss_train: 0.470578\n",
      "epoch 23, loss_val: 0.493839\n",
      "epoch 23, accuracy: 0.801425\n",
      "=================================================================================\n",
      "epoch 24, loss_train: 0.421754\n",
      "epoch 24, loss_val: 0.489634\n",
      "epoch 24, accuracy: 0.802531\n",
      "=================================================================================\n",
      "epoch 25, loss_train: 0.509294\n",
      "epoch 25, loss_val: 0.485584\n",
      "epoch 25, accuracy: 0.803329\n",
      "=================================================================================\n",
      "epoch 26, loss_train: 0.500243\n",
      "epoch 26, loss_val: 0.481702\n",
      "epoch 26, accuracy: 0.804557\n",
      "=================================================================================\n",
      "epoch 27, loss_train: 0.494326\n",
      "epoch 27, loss_val: 0.477968\n",
      "epoch 27, accuracy: 0.805295\n",
      "=================================================================================\n",
      "epoch 28, loss_train: 0.444321\n",
      "epoch 28, loss_val: 0.474357\n",
      "epoch 28, accuracy: 0.805049\n",
      "=================================================================================\n",
      "epoch 29, loss_train: 0.481124\n",
      "epoch 29, loss_val: 0.470888\n",
      "epoch 29, accuracy: 0.806154\n",
      "=================================================================================\n",
      "epoch 30, loss_train: 0.482185\n",
      "epoch 30, loss_val: 0.467528\n",
      "epoch 30, accuracy: 0.807014\n",
      "=================================================================================\n",
      "epoch 31, loss_train: 0.491194\n",
      "epoch 31, loss_val: 0.464285\n",
      "epoch 31, accuracy: 0.807383\n",
      "=================================================================================\n",
      "epoch 32, loss_train: 0.492198\n",
      "epoch 32, loss_val: 0.461144\n",
      "epoch 32, accuracy: 0.807874\n",
      "=================================================================================\n",
      "epoch 33, loss_train: 0.469108\n",
      "epoch 33, loss_val: 0.458099\n",
      "epoch 33, accuracy: 0.808857\n",
      "=================================================================================\n",
      "epoch 34, loss_train: 0.394772\n",
      "epoch 34, loss_val: 0.455162\n",
      "epoch 34, accuracy: 0.808980\n",
      "=================================================================================\n",
      "epoch 35, loss_train: 0.469546\n",
      "epoch 35, loss_val: 0.452311\n",
      "epoch 35, accuracy: 0.809164\n",
      "=================================================================================\n",
      "epoch 36, loss_train: 0.444071\n",
      "epoch 36, loss_val: 0.449540\n",
      "epoch 36, accuracy: 0.809655\n",
      "=================================================================================\n",
      "epoch 37, loss_train: 0.474478\n",
      "epoch 37, loss_val: 0.446878\n",
      "epoch 37, accuracy: 0.809778\n",
      "=================================================================================\n",
      "epoch 38, loss_train: 0.431883\n",
      "epoch 38, loss_val: 0.444272\n",
      "epoch 38, accuracy: 0.810024\n",
      "=================================================================================\n",
      "epoch 39, loss_train: 0.412570\n",
      "epoch 39, loss_val: 0.441752\n",
      "epoch 39, accuracy: 0.810822\n",
      "=================================================================================\n",
      "epoch 40, loss_train: 0.468183\n",
      "epoch 40, loss_val: 0.439306\n",
      "epoch 40, accuracy: 0.811130\n",
      "=================================================================================\n",
      "epoch 41, loss_train: 0.422439\n",
      "epoch 41, loss_val: 0.436921\n",
      "epoch 41, accuracy: 0.811682\n",
      "=================================================================================\n",
      "epoch 42, loss_train: 0.411690\n",
      "epoch 42, loss_val: 0.434611\n",
      "epoch 42, accuracy: 0.812174\n",
      "=================================================================================\n",
      "epoch 43, loss_train: 0.401888\n",
      "epoch 43, loss_val: 0.432368\n",
      "epoch 43, accuracy: 0.812419\n",
      "=================================================================================\n",
      "epoch 44, loss_train: 0.420579\n",
      "epoch 44, loss_val: 0.430183\n",
      "epoch 44, accuracy: 0.813156\n",
      "=================================================================================\n",
      "epoch 45, loss_train: 0.354930\n",
      "epoch 45, loss_val: 0.428053\n",
      "epoch 45, accuracy: 0.813648\n",
      "=================================================================================\n",
      "epoch 46, loss_train: 0.460573\n",
      "epoch 46, loss_val: 0.425997\n",
      "epoch 46, accuracy: 0.814016\n",
      "=================================================================================\n",
      "epoch 47, loss_train: 0.354107\n",
      "epoch 47, loss_val: 0.423983\n",
      "epoch 47, accuracy: 0.814262\n",
      "=================================================================================\n",
      "epoch 48, loss_train: 0.424382\n",
      "epoch 48, loss_val: 0.422032\n",
      "epoch 48, accuracy: 0.814446\n",
      "=================================================================================\n",
      "epoch 49, loss_train: 0.479453\n",
      "epoch 49, loss_val: 0.420140\n",
      "epoch 49, accuracy: 0.815060\n",
      "=================================================================================\n",
      "epoch 50, loss_train: 0.413638\n",
      "epoch 50, loss_val: 0.418285\n",
      "epoch 50, accuracy: 0.815859\n",
      "=================================================================================\n",
      "epoch 51, loss_train: 0.394969\n",
      "epoch 51, loss_val: 0.416478\n",
      "epoch 51, accuracy: 0.816289\n",
      "=================================================================================\n",
      "epoch 52, loss_train: 0.417922\n",
      "epoch 52, loss_val: 0.414723\n",
      "epoch 52, accuracy: 0.816903\n",
      "=================================================================================\n",
      "epoch 53, loss_train: 0.428869\n",
      "epoch 53, loss_val: 0.413013\n",
      "epoch 53, accuracy: 0.817395\n",
      "=================================================================================\n",
      "epoch 54, loss_train: 0.399785\n",
      "epoch 54, loss_val: 0.411334\n",
      "epoch 54, accuracy: 0.817517\n",
      "=================================================================================\n",
      "epoch 55, loss_train: 0.421011\n",
      "epoch 55, loss_val: 0.409718\n",
      "epoch 55, accuracy: 0.818009\n",
      "=================================================================================\n",
      "epoch 56, loss_train: 0.385682\n",
      "epoch 56, loss_val: 0.408140\n",
      "epoch 56, accuracy: 0.817824\n",
      "=================================================================================\n",
      "epoch 57, loss_train: 0.447774\n",
      "epoch 57, loss_val: 0.406605\n",
      "epoch 57, accuracy: 0.817456\n",
      "=================================================================================\n",
      "epoch 58, loss_train: 0.396525\n",
      "epoch 58, loss_val: 0.405101\n",
      "epoch 58, accuracy: 0.817763\n",
      "=================================================================================\n",
      "epoch 59, loss_train: 0.443948\n",
      "epoch 59, loss_val: 0.403636\n",
      "epoch 59, accuracy: 0.817763\n",
      "=================================================================================\n",
      "epoch 60, loss_train: 0.340086\n",
      "epoch 60, loss_val: 0.402214\n",
      "epoch 60, accuracy: 0.817824\n",
      "=================================================================================\n",
      "epoch 61, loss_train: 0.372543\n",
      "epoch 61, loss_val: 0.400824\n",
      "epoch 61, accuracy: 0.818070\n",
      "=================================================================================\n",
      "epoch 62, loss_train: 0.446071\n",
      "epoch 62, loss_val: 0.399467\n",
      "epoch 62, accuracy: 0.818316\n",
      "=================================================================================\n",
      "epoch 63, loss_train: 0.418911\n",
      "epoch 63, loss_val: 0.398168\n",
      "epoch 63, accuracy: 0.818684\n",
      "=================================================================================\n",
      "epoch 64, loss_train: 0.399297\n",
      "epoch 64, loss_val: 0.396877\n",
      "epoch 64, accuracy: 0.818869\n",
      "=================================================================================\n",
      "epoch 65, loss_train: 0.437136\n",
      "epoch 65, loss_val: 0.395641\n",
      "epoch 65, accuracy: 0.819053\n",
      "=================================================================================\n",
      "epoch 66, loss_train: 0.410998\n",
      "epoch 66, loss_val: 0.394420\n",
      "epoch 66, accuracy: 0.819606\n",
      "=================================================================================\n",
      "epoch 67, loss_train: 0.400117\n",
      "epoch 67, loss_val: 0.393222\n",
      "epoch 67, accuracy: 0.819913\n",
      "=================================================================================\n",
      "epoch 68, loss_train: 0.389278\n",
      "epoch 68, loss_val: 0.392069\n",
      "epoch 68, accuracy: 0.820158\n",
      "=================================================================================\n",
      "epoch 69, loss_train: 0.368638\n",
      "epoch 69, loss_val: 0.390946\n",
      "epoch 69, accuracy: 0.820097\n",
      "=================================================================================\n",
      "epoch 70, loss_train: 0.386631\n",
      "epoch 70, loss_val: 0.389839\n",
      "epoch 70, accuracy: 0.820404\n",
      "=================================================================================\n",
      "epoch 71, loss_train: 0.441518\n",
      "epoch 71, loss_val: 0.388769\n",
      "epoch 71, accuracy: 0.820711\n",
      "=================================================================================\n",
      "epoch 72, loss_train: 0.350433\n",
      "epoch 72, loss_val: 0.387712\n",
      "epoch 72, accuracy: 0.820896\n",
      "=================================================================================\n",
      "epoch 73, loss_train: 0.378331\n",
      "epoch 73, loss_val: 0.386691\n",
      "epoch 73, accuracy: 0.820834\n",
      "=================================================================================\n",
      "epoch 74, loss_train: 0.393749\n",
      "epoch 74, loss_val: 0.385702\n",
      "epoch 74, accuracy: 0.821203\n",
      "=================================================================================\n",
      "epoch 75, loss_train: 0.386056\n",
      "epoch 75, loss_val: 0.384719\n",
      "epoch 75, accuracy: 0.820957\n",
      "=================================================================================\n",
      "epoch 76, loss_train: 0.345117\n",
      "epoch 76, loss_val: 0.383775\n",
      "epoch 76, accuracy: 0.821080\n",
      "=================================================================================\n",
      "epoch 77, loss_train: 0.388845\n",
      "epoch 77, loss_val: 0.382850\n",
      "epoch 77, accuracy: 0.821387\n",
      "=================================================================================\n",
      "epoch 78, loss_train: 0.382557\n",
      "epoch 78, loss_val: 0.381948\n",
      "epoch 78, accuracy: 0.821694\n",
      "=================================================================================\n",
      "epoch 79, loss_train: 0.377936\n",
      "epoch 79, loss_val: 0.381073\n",
      "epoch 79, accuracy: 0.822001\n",
      "=================================================================================\n",
      "epoch 80, loss_train: 0.403439\n",
      "epoch 80, loss_val: 0.380211\n",
      "epoch 80, accuracy: 0.822001\n",
      "=================================================================================\n",
      "epoch 81, loss_train: 0.414208\n",
      "epoch 81, loss_val: 0.379361\n",
      "epoch 81, accuracy: 0.821694\n",
      "=================================================================================\n",
      "epoch 82, loss_train: 0.365561\n",
      "epoch 82, loss_val: 0.378538\n",
      "epoch 82, accuracy: 0.822063\n",
      "=================================================================================\n",
      "epoch 83, loss_train: 0.428138\n",
      "epoch 83, loss_val: 0.377731\n",
      "epoch 83, accuracy: 0.822370\n",
      "=================================================================================\n",
      "epoch 84, loss_train: 0.411364\n",
      "epoch 84, loss_val: 0.376946\n",
      "epoch 84, accuracy: 0.822615\n",
      "=================================================================================\n",
      "epoch 85, loss_train: 0.388477\n",
      "epoch 85, loss_val: 0.376181\n",
      "epoch 85, accuracy: 0.822861\n",
      "=================================================================================\n",
      "epoch 86, loss_train: 0.347627\n",
      "epoch 86, loss_val: 0.375434\n",
      "epoch 86, accuracy: 0.822861\n",
      "=================================================================================\n",
      "epoch 87, loss_train: 0.417309\n",
      "epoch 87, loss_val: 0.374702\n",
      "epoch 87, accuracy: 0.822738\n",
      "=================================================================================\n",
      "epoch 88, loss_train: 0.409477\n",
      "epoch 88, loss_val: 0.373992\n",
      "epoch 88, accuracy: 0.823107\n",
      "=================================================================================\n",
      "epoch 89, loss_train: 0.451091\n",
      "epoch 89, loss_val: 0.373290\n",
      "epoch 89, accuracy: 0.823291\n",
      "=================================================================================\n",
      "epoch 90, loss_train: 0.352646\n",
      "epoch 90, loss_val: 0.372599\n",
      "epoch 90, accuracy: 0.823352\n",
      "=================================================================================\n",
      "epoch 91, loss_train: 0.330806\n",
      "epoch 91, loss_val: 0.371931\n",
      "epoch 91, accuracy: 0.823659\n",
      "=================================================================================\n",
      "epoch 92, loss_train: 0.389550\n",
      "epoch 92, loss_val: 0.371281\n",
      "epoch 92, accuracy: 0.823721\n",
      "=================================================================================\n",
      "epoch 93, loss_train: 0.320766\n",
      "epoch 93, loss_val: 0.370647\n",
      "epoch 93, accuracy: 0.824089\n",
      "=================================================================================\n",
      "epoch 94, loss_train: 0.385275\n",
      "epoch 94, loss_val: 0.370024\n",
      "epoch 94, accuracy: 0.824089\n",
      "=================================================================================\n",
      "epoch 95, loss_train: 0.392917\n",
      "epoch 95, loss_val: 0.369417\n",
      "epoch 95, accuracy: 0.824274\n",
      "=================================================================================\n",
      "epoch 96, loss_train: 0.327034\n",
      "epoch 96, loss_val: 0.368810\n",
      "epoch 96, accuracy: 0.824274\n",
      "=================================================================================\n",
      "epoch 97, loss_train: 0.354672\n",
      "epoch 97, loss_val: 0.368222\n",
      "epoch 97, accuracy: 0.824274\n",
      "=================================================================================\n",
      "epoch 98, loss_train: 0.376658\n",
      "epoch 98, loss_val: 0.367653\n",
      "epoch 98, accuracy: 0.824642\n",
      "=================================================================================\n",
      "epoch 99, loss_train: 0.391497\n",
      "epoch 99, loss_val: 0.367089\n",
      "epoch 99, accuracy: 0.824519\n",
      "=================================================================================\n",
      "epoch 100, loss_train: 0.338787\n",
      "epoch 100, loss_val: 0.366543\n",
      "epoch 100, accuracy: 0.824642\n",
      "=================================================================================\n",
      "epoch 101, loss_train: 0.322814\n",
      "epoch 101, loss_val: 0.366008\n",
      "epoch 101, accuracy: 0.824581\n",
      "=================================================================================\n",
      "epoch 102, loss_train: 0.373501\n",
      "epoch 102, loss_val: 0.365489\n",
      "epoch 102, accuracy: 0.824704\n",
      "=================================================================================\n",
      "epoch 103, loss_train: 0.368725\n",
      "epoch 103, loss_val: 0.364978\n",
      "epoch 103, accuracy: 0.824704\n",
      "=================================================================================\n",
      "epoch 104, loss_train: 0.352375\n",
      "epoch 104, loss_val: 0.364476\n",
      "epoch 104, accuracy: 0.824888\n",
      "=================================================================================\n",
      "epoch 105, loss_train: 0.357591\n",
      "epoch 105, loss_val: 0.363978\n",
      "epoch 105, accuracy: 0.825195\n",
      "=================================================================================\n",
      "epoch 106, loss_train: 0.337646\n",
      "epoch 106, loss_val: 0.363500\n",
      "epoch 106, accuracy: 0.825379\n",
      "=================================================================================\n",
      "epoch 107, loss_train: 0.346477\n",
      "epoch 107, loss_val: 0.363024\n",
      "epoch 107, accuracy: 0.825379\n",
      "=================================================================================\n",
      "epoch 108, loss_train: 0.382867\n",
      "epoch 108, loss_val: 0.362558\n",
      "epoch 108, accuracy: 0.825318\n",
      "=================================================================================\n",
      "epoch 109, loss_train: 0.370691\n",
      "epoch 109, loss_val: 0.362097\n",
      "epoch 109, accuracy: 0.825256\n",
      "=================================================================================\n",
      "epoch 110, loss_train: 0.326965\n",
      "epoch 110, loss_val: 0.361655\n",
      "epoch 110, accuracy: 0.825318\n",
      "=================================================================================\n",
      "epoch 111, loss_train: 0.340040\n",
      "epoch 111, loss_val: 0.361219\n",
      "epoch 111, accuracy: 0.825318\n",
      "=================================================================================\n",
      "epoch 112, loss_train: 0.345994\n",
      "epoch 112, loss_val: 0.360787\n",
      "epoch 112, accuracy: 0.825441\n",
      "=================================================================================\n",
      "epoch 113, loss_train: 0.342675\n",
      "epoch 113, loss_val: 0.360370\n",
      "epoch 113, accuracy: 0.825502\n",
      "=================================================================================\n",
      "epoch 114, loss_train: 0.386643\n",
      "epoch 114, loss_val: 0.359958\n",
      "epoch 114, accuracy: 0.825686\n",
      "=================================================================================\n",
      "epoch 115, loss_train: 0.376623\n",
      "epoch 115, loss_val: 0.359554\n",
      "epoch 115, accuracy: 0.825932\n",
      "=================================================================================\n",
      "epoch 116, loss_train: 0.381514\n",
      "epoch 116, loss_val: 0.359152\n",
      "epoch 116, accuracy: 0.825748\n",
      "=================================================================================\n",
      "epoch 117, loss_train: 0.401017\n",
      "epoch 117, loss_val: 0.358760\n",
      "epoch 117, accuracy: 0.825686\n",
      "=================================================================================\n",
      "epoch 118, loss_train: 0.357927\n",
      "epoch 118, loss_val: 0.358382\n",
      "epoch 118, accuracy: 0.825502\n",
      "=================================================================================\n",
      "epoch 119, loss_train: 0.381329\n",
      "epoch 119, loss_val: 0.357999\n",
      "epoch 119, accuracy: 0.825625\n",
      "=================================================================================\n",
      "epoch 120, loss_train: 0.403113\n",
      "epoch 120, loss_val: 0.357629\n",
      "epoch 120, accuracy: 0.825871\n",
      "=================================================================================\n",
      "epoch 121, loss_train: 0.318685\n",
      "epoch 121, loss_val: 0.357266\n",
      "epoch 121, accuracy: 0.826055\n",
      "=================================================================================\n",
      "epoch 122, loss_train: 0.412532\n",
      "epoch 122, loss_val: 0.356917\n",
      "epoch 122, accuracy: 0.826423\n",
      "=================================================================================\n",
      "epoch 123, loss_train: 0.336277\n",
      "epoch 123, loss_val: 0.356579\n",
      "epoch 123, accuracy: 0.826485\n",
      "=================================================================================\n",
      "epoch 124, loss_train: 0.406404\n",
      "epoch 124, loss_val: 0.356226\n",
      "epoch 124, accuracy: 0.826485\n",
      "=================================================================================\n",
      "epoch 125, loss_train: 0.371960\n",
      "epoch 125, loss_val: 0.355886\n",
      "epoch 125, accuracy: 0.826669\n",
      "=================================================================================\n",
      "epoch 126, loss_train: 0.376037\n",
      "epoch 126, loss_val: 0.355555\n",
      "epoch 126, accuracy: 0.826608\n",
      "=================================================================================\n",
      "epoch 127, loss_train: 0.349353\n",
      "epoch 127, loss_val: 0.355230\n",
      "epoch 127, accuracy: 0.826853\n",
      "=================================================================================\n",
      "epoch 128, loss_train: 0.409738\n",
      "epoch 128, loss_val: 0.354910\n",
      "epoch 128, accuracy: 0.826792\n",
      "=================================================================================\n",
      "epoch 129, loss_train: 0.350494\n",
      "epoch 129, loss_val: 0.354595\n",
      "epoch 129, accuracy: 0.826731\n",
      "=================================================================================\n",
      "epoch 130, loss_train: 0.313579\n",
      "epoch 130, loss_val: 0.354289\n",
      "epoch 130, accuracy: 0.826608\n",
      "=================================================================================\n",
      "epoch 131, loss_train: 0.319229\n",
      "epoch 131, loss_val: 0.353984\n",
      "epoch 131, accuracy: 0.826423\n",
      "=================================================================================\n",
      "epoch 132, loss_train: 0.378289\n",
      "epoch 132, loss_val: 0.353690\n",
      "epoch 132, accuracy: 0.826423\n",
      "=================================================================================\n",
      "epoch 133, loss_train: 0.304862\n",
      "epoch 133, loss_val: 0.353398\n",
      "epoch 133, accuracy: 0.826546\n",
      "=================================================================================\n",
      "epoch 134, loss_train: 0.365304\n",
      "epoch 134, loss_val: 0.353106\n",
      "epoch 134, accuracy: 0.825993\n",
      "=================================================================================\n",
      "epoch 135, loss_train: 0.367071\n",
      "epoch 135, loss_val: 0.352827\n",
      "epoch 135, accuracy: 0.826116\n",
      "=================================================================================\n",
      "epoch 136, loss_train: 0.327523\n",
      "epoch 136, loss_val: 0.352544\n",
      "epoch 136, accuracy: 0.825932\n",
      "=================================================================================\n",
      "epoch 137, loss_train: 0.341041\n",
      "epoch 137, loss_val: 0.352269\n",
      "epoch 137, accuracy: 0.825625\n",
      "=================================================================================\n",
      "epoch 138, loss_train: 0.353648\n",
      "epoch 138, loss_val: 0.352007\n",
      "epoch 138, accuracy: 0.825871\n",
      "=================================================================================\n",
      "epoch 139, loss_train: 0.394824\n",
      "epoch 139, loss_val: 0.351745\n",
      "epoch 139, accuracy: 0.825686\n",
      "=================================================================================\n",
      "epoch 140, loss_train: 0.385348\n",
      "epoch 140, loss_val: 0.351487\n",
      "epoch 140, accuracy: 0.825871\n",
      "=================================================================================\n",
      "epoch 141, loss_train: 0.365367\n",
      "epoch 141, loss_val: 0.351220\n",
      "epoch 141, accuracy: 0.825564\n",
      "=================================================================================\n",
      "epoch 142, loss_train: 0.347701\n",
      "epoch 142, loss_val: 0.350965\n",
      "epoch 142, accuracy: 0.825686\n",
      "=================================================================================\n",
      "epoch 143, loss_train: 0.406582\n",
      "epoch 143, loss_val: 0.350722\n",
      "epoch 143, accuracy: 0.825809\n",
      "=================================================================================\n",
      "epoch 144, loss_train: 0.342527\n",
      "epoch 144, loss_val: 0.350472\n",
      "epoch 144, accuracy: 0.825871\n",
      "=================================================================================\n",
      "epoch 145, loss_train: 0.308495\n",
      "epoch 145, loss_val: 0.350227\n",
      "epoch 145, accuracy: 0.825871\n",
      "=================================================================================\n",
      "epoch 146, loss_train: 0.309416\n",
      "epoch 146, loss_val: 0.349990\n",
      "epoch 146, accuracy: 0.826055\n",
      "=================================================================================\n",
      "epoch 147, loss_train: 0.331285\n",
      "epoch 147, loss_val: 0.349761\n",
      "epoch 147, accuracy: 0.826362\n",
      "=================================================================================\n",
      "epoch 148, loss_train: 0.355453\n",
      "epoch 148, loss_val: 0.349529\n",
      "epoch 148, accuracy: 0.826116\n",
      "=================================================================================\n",
      "epoch 149, loss_train: 0.351317\n",
      "epoch 149, loss_val: 0.349309\n",
      "epoch 149, accuracy: 0.826423\n",
      "=================================================================================\n",
      "epoch 150, loss_train: 0.340426\n",
      "epoch 150, loss_val: 0.349088\n",
      "epoch 150, accuracy: 0.826239\n",
      "=================================================================================\n",
      "epoch 151, loss_train: 0.371621\n",
      "epoch 151, loss_val: 0.348877\n",
      "epoch 151, accuracy: 0.826608\n",
      "=================================================================================\n",
      "epoch 152, loss_train: 0.349790\n",
      "epoch 152, loss_val: 0.348652\n",
      "epoch 152, accuracy: 0.826423\n",
      "=================================================================================\n",
      "epoch 153, loss_train: 0.341530\n",
      "epoch 153, loss_val: 0.348441\n",
      "epoch 153, accuracy: 0.826423\n",
      "=================================================================================\n",
      "epoch 154, loss_train: 0.375366\n",
      "epoch 154, loss_val: 0.348230\n",
      "epoch 154, accuracy: 0.826423\n",
      "=================================================================================\n",
      "epoch 155, loss_train: 0.336195\n",
      "epoch 155, loss_val: 0.348023\n",
      "epoch 155, accuracy: 0.826423\n",
      "=================================================================================\n",
      "epoch 156, loss_train: 0.312923\n",
      "epoch 156, loss_val: 0.347815\n",
      "epoch 156, accuracy: 0.826485\n",
      "=================================================================================\n",
      "epoch 157, loss_train: 0.363026\n",
      "epoch 157, loss_val: 0.347618\n",
      "epoch 157, accuracy: 0.826546\n",
      "=================================================================================\n",
      "epoch 158, loss_train: 0.348532\n",
      "epoch 158, loss_val: 0.347421\n",
      "epoch 158, accuracy: 0.826731\n",
      "=================================================================================\n",
      "epoch 159, loss_train: 0.392849\n",
      "epoch 159, loss_val: 0.347224\n",
      "epoch 159, accuracy: 0.826915\n",
      "=================================================================================\n",
      "epoch 160, loss_train: 0.357335\n",
      "epoch 160, loss_val: 0.347039\n",
      "epoch 160, accuracy: 0.827160\n",
      "=================================================================================\n",
      "epoch 161, loss_train: 0.365645\n",
      "epoch 161, loss_val: 0.346844\n",
      "epoch 161, accuracy: 0.827038\n",
      "=================================================================================\n",
      "epoch 162, loss_train: 0.316401\n",
      "epoch 162, loss_val: 0.346651\n",
      "epoch 162, accuracy: 0.826608\n",
      "=================================================================================\n",
      "epoch 163, loss_train: 0.352777\n",
      "epoch 163, loss_val: 0.346473\n",
      "epoch 163, accuracy: 0.826853\n",
      "=================================================================================\n",
      "epoch 164, loss_train: 0.357224\n",
      "epoch 164, loss_val: 0.346300\n",
      "epoch 164, accuracy: 0.827283\n",
      "=================================================================================\n",
      "epoch 165, loss_train: 0.372532\n",
      "epoch 165, loss_val: 0.346112\n",
      "epoch 165, accuracy: 0.826976\n",
      "=================================================================================\n",
      "epoch 166, loss_train: 0.361683\n",
      "epoch 166, loss_val: 0.345932\n",
      "epoch 166, accuracy: 0.826853\n",
      "=================================================================================\n",
      "epoch 167, loss_train: 0.335258\n",
      "epoch 167, loss_val: 0.345761\n",
      "epoch 167, accuracy: 0.827222\n",
      "=================================================================================\n",
      "epoch 168, loss_train: 0.338968\n",
      "epoch 168, loss_val: 0.345594\n",
      "epoch 168, accuracy: 0.827345\n",
      "=================================================================================\n",
      "epoch 169, loss_train: 0.359854\n",
      "epoch 169, loss_val: 0.345435\n",
      "epoch 169, accuracy: 0.827713\n",
      "=================================================================================\n",
      "epoch 170, loss_train: 0.354664\n",
      "epoch 170, loss_val: 0.345267\n",
      "epoch 170, accuracy: 0.827775\n",
      "=================================================================================\n",
      "epoch 171, loss_train: 0.357215\n",
      "epoch 171, loss_val: 0.345094\n",
      "epoch 171, accuracy: 0.827590\n",
      "=================================================================================\n",
      "epoch 172, loss_train: 0.356271\n",
      "epoch 172, loss_val: 0.344931\n",
      "epoch 172, accuracy: 0.827406\n",
      "=================================================================================\n",
      "epoch 173, loss_train: 0.359958\n",
      "epoch 173, loss_val: 0.344769\n",
      "epoch 173, accuracy: 0.827468\n",
      "=================================================================================\n",
      "epoch 174, loss_train: 0.389217\n",
      "epoch 174, loss_val: 0.344610\n",
      "epoch 174, accuracy: 0.827468\n",
      "=================================================================================\n",
      "epoch 175, loss_train: 0.342286\n",
      "epoch 175, loss_val: 0.344458\n",
      "epoch 175, accuracy: 0.827529\n",
      "=================================================================================\n",
      "epoch 176, loss_train: 0.361741\n",
      "epoch 176, loss_val: 0.344308\n",
      "epoch 176, accuracy: 0.827652\n",
      "=================================================================================\n",
      "epoch 177, loss_train: 0.371928\n",
      "epoch 177, loss_val: 0.344161\n",
      "epoch 177, accuracy: 0.827836\n",
      "=================================================================================\n",
      "epoch 178, loss_train: 0.373566\n",
      "epoch 178, loss_val: 0.344016\n",
      "epoch 178, accuracy: 0.827959\n",
      "=================================================================================\n",
      "epoch 179, loss_train: 0.337972\n",
      "epoch 179, loss_val: 0.343860\n",
      "epoch 179, accuracy: 0.827898\n",
      "=================================================================================\n",
      "epoch 180, loss_train: 0.311447\n",
      "epoch 180, loss_val: 0.343709\n",
      "epoch 180, accuracy: 0.827959\n",
      "=================================================================================\n",
      "epoch 181, loss_train: 0.354169\n",
      "epoch 181, loss_val: 0.343563\n",
      "epoch 181, accuracy: 0.827959\n",
      "=================================================================================\n",
      "epoch 182, loss_train: 0.371060\n",
      "epoch 182, loss_val: 0.343421\n",
      "epoch 182, accuracy: 0.828143\n",
      "=================================================================================\n",
      "epoch 183, loss_train: 0.353948\n",
      "epoch 183, loss_val: 0.343277\n",
      "epoch 183, accuracy: 0.828266\n",
      "=================================================================================\n",
      "epoch 184, loss_train: 0.363700\n",
      "epoch 184, loss_val: 0.343140\n",
      "epoch 184, accuracy: 0.828266\n",
      "=================================================================================\n",
      "epoch 185, loss_train: 0.326593\n",
      "epoch 185, loss_val: 0.343002\n",
      "epoch 185, accuracy: 0.828512\n",
      "=================================================================================\n",
      "epoch 186, loss_train: 0.368279\n",
      "epoch 186, loss_val: 0.342872\n",
      "epoch 186, accuracy: 0.828327\n",
      "=================================================================================\n",
      "epoch 187, loss_train: 0.395126\n",
      "epoch 187, loss_val: 0.342739\n",
      "epoch 187, accuracy: 0.828450\n",
      "=================================================================================\n",
      "epoch 188, loss_train: 0.341038\n",
      "epoch 188, loss_val: 0.342614\n",
      "epoch 188, accuracy: 0.828696\n",
      "=================================================================================\n",
      "epoch 189, loss_train: 0.327644\n",
      "epoch 189, loss_val: 0.342477\n",
      "epoch 189, accuracy: 0.828512\n",
      "=================================================================================\n",
      "epoch 190, loss_train: 0.299399\n",
      "epoch 190, loss_val: 0.342348\n",
      "epoch 190, accuracy: 0.828450\n",
      "=================================================================================\n",
      "epoch 191, loss_train: 0.307375\n",
      "epoch 191, loss_val: 0.342218\n",
      "epoch 191, accuracy: 0.828266\n",
      "=================================================================================\n",
      "epoch 192, loss_train: 0.346177\n",
      "epoch 192, loss_val: 0.342101\n",
      "epoch 192, accuracy: 0.828512\n",
      "=================================================================================\n",
      "epoch 193, loss_train: 0.331322\n",
      "epoch 193, loss_val: 0.341973\n",
      "epoch 193, accuracy: 0.828450\n",
      "=================================================================================\n",
      "epoch 194, loss_train: 0.332837\n",
      "epoch 194, loss_val: 0.341850\n",
      "epoch 194, accuracy: 0.828266\n",
      "=================================================================================\n",
      "epoch 195, loss_train: 0.335720\n",
      "epoch 195, loss_val: 0.341731\n",
      "epoch 195, accuracy: 0.828389\n",
      "=================================================================================\n",
      "epoch 196, loss_train: 0.303732\n",
      "epoch 196, loss_val: 0.341608\n",
      "epoch 196, accuracy: 0.828512\n",
      "=================================================================================\n",
      "epoch 197, loss_train: 0.308071\n",
      "epoch 197, loss_val: 0.341491\n",
      "epoch 197, accuracy: 0.828573\n",
      "=================================================================================\n",
      "epoch 198, loss_train: 0.334013\n",
      "epoch 198, loss_val: 0.341371\n",
      "epoch 198, accuracy: 0.828573\n",
      "=================================================================================\n",
      "epoch 199, loss_train: 0.302218\n",
      "epoch 199, loss_val: 0.341253\n",
      "epoch 199, accuracy: 0.828635\n",
      "=================================================================================\n",
      "epoch 200, loss_train: 0.324133\n",
      "epoch 200, loss_val: 0.341140\n",
      "epoch 200, accuracy: 0.828696\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 200\n",
    "model =logisreg\n",
    "loss = cost\n",
    "batch_size = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, x_train, y_train):\n",
    "        l = loss(model(X, theta), y).sum()   # 计算损失函数\n",
    "        theta = step_gradient(theta, X, y, lr)\n",
    "    # 验证\n",
    "    l_val = loss(model(x_val, theta), y_val).sum()\n",
    "    print(f'epoch {epoch + 1}, loss_train: {l:f}')  # 训练集损失函数值\n",
    "    print(f'epoch {epoch + 1}, loss_val: {l_val:f}')  # 验证集瞬时函数值\n",
    "    print(f'epoch {epoch + 1}, accuracy: {evaluate_accuracy(model(x_val, theta), y_val):f}')  # 验证集准确度\n",
    "    print(\"=================================================================================\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}