{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 7.1 词向量 Word2Vec\n",
    "\n",
    "从本小节开始，我们正式进入自然语言处理器（NLP）的学习，强烈推荐斯坦福的[CS224n](https://web.stanford.edu/class/cs224n/)课程。\n",
    "\n",
    "在自然语言处理系统当中，**词向量**是用于表示单词意义的向量， 并且还可以被认为是单词的特征向量或表示。 将单词映射到实向量的技术称为词嵌入。 近年来，词嵌入逐渐成为自然语言处理的基础知识。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.1.1 词向量模型\n",
    "\n",
    "独热向量是最为容易构建的，但是它并不是一个好的选择。一个主要原因是独热向量**不能准确表达不同词之间的相似度**，比如我们经常使用的“余弦相似度”。对于向量，它们的余弦相似度是它们之间角度的余弦：\n",
    "$$\n",
    "\\frac{\\mathbf{x}^{\\top} \\mathbf{y}}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|} \\in[-1,1]\n",
    "$$\n",
    "由于任意两个不同词的**独热向量之间的余弦相似度为0**，所以独热向量不能编码词之间的相似性。\n",
    "\n",
    "word2vec工具是为了解决上述问题而提出的。它将每个词映射到一个**固定长度的向量**，这些向量能更好地表达不同词之间的相似性和类比关系。\n",
    "word2vec工具包含两个模型，即**跳元模型**（skip-gram）和**连续词袋**（CBOW） 。对于在语义上有意义的表示，它们的训练依赖于条件概率，条件概率可以被看作是使用语料库中一些词来预测另一些单词。由于是不带标签的数据，因此跳元模型和连续词袋都是自监督模型。\n",
    "\n",
    "* **跳元模型(Skip-Gram)**\n",
    "\n",
    "跳元模型假设一个词可以用来在文本序列中生成其周围的单词。以文本序列“the”、“man”、“loves”、“his”、“son”为例。假设中心词选择“loves”，并将上下文窗口设置为2\n",
    "给定中心词“loves”，跳元模型考虑生成上下文词“the”、“man”、“him”、“son”的条件概率：\n",
    "$$\n",
    "P(“the”、“man”、“him”、“son” | \"loves\")\n",
    "$$\n",
    "假设上下文词是在给定中心词的情况下独立生成的（即条件独立性）。在这种情况下，上述条件概率可以重写为：\n",
    "$$\n",
    "P(\\text { \"the\" |\"loves\" }) \\cdot P(\" \\text { man\" | \"loves\" }) \\cdot P(\\text { \"his\" | \"loves\" }) \\cdot P(\\text { \"son\" | \"loves\" }) .\n",
    "$$\n",
    "![](https://zh.d2l.ai/_images/skip-gram.svg)\n",
    "\n",
    "\n",
    "![image-20220609195244586](https://s2.loli.net/2022/06/09/x8m29HSwMNOfjIy.png)\n",
    "目标函数为：\n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{T} \\log L(\\theta)=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} \\log P\\left(w_{t+j} \\mid w_{t} ; \\theta\\right)\n",
    "$$\n",
    "\n",
    "* **如何计算** $P(w_{t+j} \\mid w_{t};\\theta)$?\n",
    "\n",
    "  * 每个单词 $w$ 使用两个向量来计算\n",
    "\n",
    "    * $v_w$ when $w$ is a ==center word==\n",
    "    * $u_w$ when $w$ is a ==context word==\n",
    "\n",
    "  * center word $c$ 和 context word $o$:\n",
    "\n",
    "    * $$\n",
    "      P(o \\mid c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\n",
    "      $$\n",
    "\n",
    "\n",
    "\n",
    "  ![image-20220609195936885](https://s2.loli.net/2022/06/09/4V92XnBD5uNScfp.png)\n",
    "\n",
    "**预测函数**\n",
    "\n",
    "![image-20220609200429388](https://s2.loli.net/2022/06/09/IyS8Zct7h6qKRp5.png)\n",
    "\n",
    "![image-20220609201213536](https://s2.loli.net/2022/06/09/sY1gt9kKyZzuQP4.png)\n",
    "\n",
    "梯度\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mathcal{U}_{\\text {new }} \\leftarrow \\mathcal{U}_{\\text {old }}-\\alpha \\nabla_{\\mathcal{U}} J \\\\\n",
    "&\\mathcal{V}_{\\text {old }} \\leftarrow \\mathcal{V}_{\\text {old }}-\\alpha \\nabla_{\\mathcal{V}} J\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "简单实践：\n",
    "\n",
    "[Genism Package for word2vec](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "\n",
    "* 但这个包实际上在深度学习中**不常用**\n",
    "\n",
    "[Gensim word vector visualization notebook](https://www.kaggle.com/code/yixuanzhou94/gensim-word-vector-visualization/notebook)\n",
    "\n",
    "* **连续词袋（CBOW）模型**\n",
    "\n",
    "*连续词袋*（CBOW）模型类似于跳元模型。与跳元模型的主要区别在于，连续词袋模型假设中心词是基于其在文本序列中的周围上下文词生成的。例如，在文本序列“the”、“man”、“loves”、“his”、“son”中，在“loves”为中心词且上下文窗口为2的情况下，连续词袋模型考虑基于上下文词“the”、“man”、“him”、“son”生成中心词“loves”的条件概率，即：\n",
    "\n",
    "$$P(\\textrm{\"loves\"}\\mid\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}).$$\n",
    "\n",
    "![连续词袋模型考虑了给定周围上下文词生成中心词条件概率](http://d2l.ai/_images/cbow.svg)\n",
    "\n",
    "由于连续词袋模型中存在多个上下文词，因此在计算条件概率时对这些上下文词向量进行平均。具体地说，对于字典中索引$i$的任意词，分别用$\\mathbf{v}_i\\in\\mathbb{R}^d$和$\\mathbf{u}_i\\in\\mathbb{R}^d$表示用作*上下文*词和*中心*词的两个向量（符号与跳元模型中相反）。给定上下文词$w_{o_1}, \\ldots, w_{o_{2m}}$（在词表中索引是$o_1, \\ldots, o_{2m}$）生成任意中心词$w_c$（在词表中索引是$c$）的条件概率可以由以下公式建模:\n",
    "\n",
    "$$P(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\text{exp}\\left(\\frac{1}{2m}\\mathbf{u}_c^\\top (\\mathbf{v}_{o_1} + \\ldots, + \\mathbf{v}_{o_{2m}}) \\right)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}\\left(\\frac{1}{2m}\\mathbf{u}_i^\\top (\\mathbf{v}_{o_1} + \\ldots, + \\mathbf{v}_{o_{2m}}) \\right)}.$$\n",
    "\n",
    "\n",
    "为了简洁起见，我们设为$\\mathcal{W}_o= \\{w_{o_1}, \\ldots, w_{o_{2m}}\\}$和$\\bar{\\mathbf{v}}_o = \\left(\\mathbf{v}_{o_1} + \\ldots, + \\mathbf{v}_{o_{2m}} \\right)/(2m)$。那么可以简化为：\n",
    "\n",
    "$$P(w_c \\mid \\mathcal{W}_o) = \\frac{\\exp\\left(\\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)}.$$\n",
    "\n",
    "给定长度为$T$的文本序列，其中时间步$t$处的词表示为$w^{(t)}$。对于上下文窗口$m$，连续词袋模型的似然函数是在给定其上下文词的情况下生成所有中心词的概率：\n",
    "\n",
    "$$ \\prod_{t=1}^{T}  P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练连续词袋模型与训练跳元模型几乎是一样的。连续词袋模型的最大似然估计等价于最小化以下损失函数：\n",
    "\n",
    "$$  -\\sum_{t=1}^T  \\text{log}\\, P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).$$\n",
    "\n",
    "请注意，\n",
    "\n",
    "$$\\log\\,P(w_c \\mid \\mathcal{W}_o) = \\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o - \\log\\,\\left(\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)\\right).$$\n",
    "\n",
    "通过微分，我们可以获得其关于任意上下文词向量$\\mathbf{v}_{o_i}$（$i = 1, \\ldots, 2m$）的梯度，如下：\n",
    "\n",
    "$$\\frac{\\partial \\log\\, P(w_c \\mid \\mathcal{W}_o)}{\\partial \\mathbf{v}_{o_i}} = \\frac{1}{2m} \\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} \\frac{\\exp(\\mathbf{u}_j^\\top \\bar{\\mathbf{v}}_o)\\mathbf{u}_j}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o)} \\right) = \\frac{1}{2m}\\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\mathbf{u}_j \\right).$$\n",
    "\n",
    "其他词向量的梯度可以以相同的方式获得。与跳元模型不同，连续词袋模型通常使用上下文词向量作为词表示。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**小结:**\n",
    "* 词向量是用于表示单词意义的向量，也可以看作是词的特征向量。将词映射到实向量的技术称为词嵌入。\n",
    "* word2vec工具包含跳元模型和连续词袋模型。\n",
    "* 跳元模型假设一个单词可用于在文本序列中，生成其周围的单词；而连续词袋模型假设基于上下文词来生成中心单词。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7.1.2 采样与近似训练\n",
    "\n",
    "为了降低上述计算复杂度，本节将介绍两种近似训练方法：负采样和分层softmax。 由于跳元模型和连续词袋模型的相似性，我们将以跳元模型为例来描述这两种近似训练方法。\n",
    "\n",
    "#### 7.1.2.1 负采样\n",
    "负采样修改了原目标函数。给定中心词$w_c$的上下文窗口，任意上下文词$w_o$来自该上下文窗口的被认为是由下式建模概率的事件：\n",
    "$$P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c),$$\n",
    "\n",
    "其中$\\sigma$使用了sigmoid激活函数的定义：\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+\\exp(-x)}.$$\n",
    "\n",
    "让我们从最大化文本序列中所有这些事件的联合概率开始训练词嵌入。具体而言，给定长度为$T$的文本序列，以$w^{(t)}$表示时间步$t$的词，并使上下文窗口为$m$，考虑最大化联合概率：\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}).$$\n",
    "\n",
    "然而， 上面式子只考虑那些正样本的事件。仅当所有词向量都等于无穷大时，其中的联合概率才最大化为1。当然，这样的结果毫无意义。为了使目标函数更有意义，*负采样*添加从预定义分布中采样的负样本。\n",
    "\n",
    "用$S$表示上下文词$w_o$来自中心词$w_c$的上下文窗口的事件。对于这个涉及$w_o$的事件，从预定义分布$P(w)$中采样$K$个不是来自这个上下文窗口*噪声词*。用$N_k$表示噪声词$w_k$（$k=1, \\ldots, K$）不是来自$w_c$的上下文窗口的事件。假设正例和负例$S, N_1, \\ldots, N_K$的这些事件是相互独立的。负采样将公式中的联合概率（仅涉及正例）重写为\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),$$\n",
    "\n",
    "通过事件$S, N_1, \\ldots, N_K$近似条件概率：\n",
    "\n",
    "$$ P(w^{(t+j)} \\mid w^{(t)}) =P(D=1\\mid w^{(t)}, w^{(t+j)})\\prod_{k=1,\\ w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k).$$\n",
    "\n",
    "分别用$i_t$和$h_k$表示词$w^{(t)}$和噪声词$w_k$在文本序列的时间步$t$处的索引。\n",
    "\n",
    "关于条件概率的对数损失为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\log P(w^{(t+j)} \\mid w^{(t)})\n",
    "=& -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "我们可以看到，现在每个训练步的梯度计算成本与词表大小无关，而是线性依赖于$K$。当将超参数$K$设置为较小的值时，在负采样的每个训练步处的梯度的计算成本较小。\n",
    "\n",
    "### 7.1.2.2 层序`Softmax`\n",
    "\n",
    "![](https://zh.d2l.ai/_images/hi-softmax.svg)\n",
    "\n",
    "用于近似训练的分层softmax，其中书的每个叶子节点表示词表中的一个词。\n",
    "\n",
    "*层序Softmax*（hierarchical softmax）使用二叉树, 其中树的每个叶节点表示词表$\\mathcal{V}$中的一个词。\n",
    "用$L(w)$表示二叉树中表示字$w$的从根节点到叶节点的路径上的节点数（包括两端）。设$n(w,j)$为该路径上的$j^\\mathrm{th}$节点，其上下文字向量为$\\mathbf{u}_{n(w, j)}$。\n",
    "条件概率近似为\n",
    "\n",
    "$$P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [\\![  n(w_o, j+1) = \\text{leftChild}(n(w_o, j)) ]\\!] \\cdot \\mathbf{u}_{n(w_o, j)}^\\top \\mathbf{v}_c\\right),$$\n",
    "\n",
    "其中函数$\\sigma$在式子中定义，$\\text{leftChild}(n)$是节点$n$的左子节点：如果$x$为真，$[\\![x]\\!] = 1$;否则$[\\![x]\\!] = -1$。\n",
    "\n",
    "为了说明，让我们计算中给定词$w_c$生成词$w_3$的条件概率。这需要$w_c$的词向量$\\mathbf{v}_c$和从根到$w_3$的路径（图中加粗的路径）上的非叶节点向量之间的点积，该路径依次向左、向右和向左遍历：\n",
    "\n",
    "$$P(w_3 \\mid w_c) = \\sigma(\\mathbf{u}_{n(w_3, 1)}^\\top \\mathbf{v}_c) \\cdot \\sigma(-\\mathbf{u}_{n(w_3, 2)}^\\top \\mathbf{v}_c) \\cdot \\sigma(\\mathbf{u}_{n(w_3, 3)}^\\top \\mathbf{v}_c).$$\n",
    "\n",
    "由$\\sigma(x)+\\sigma(-x) = 1$，它认为基于任意词$w_c$生成词表$\\mathcal{V}$中所有词的条件概率总和为1：\n",
    "\n",
    "$$\\sum_{w \\in \\mathcal{V}} P(w \\mid w_c) = 1.$$\n",
    "\n",
    "幸运的是，由于二叉树结构，$L(w_o)-1$大约与$\\mathcal{O}(\\text{log}_2|\\mathcal{V}|)$是一个数量级。当词表大小$\\mathcal{V}$很大时，与没有近似训练的相比，使用分层softmax的每个训练步的计算代价显著降低。\n",
    "\n",
    "**小结：**\n",
    "* 负采样通过考虑相互独立的事件来构造损失函数，这些事件同时涉及正例和负例。训练的计算量与每一步的噪声词数成线性关系。\n",
    "* 分层softmax使用二叉树中从根节点到叶节点的路径构造损失函数。训练的计算成本取决于词表大小的对数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}