{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.6 序列到序列(Seq2Seq)学习模型\n",
    " 机器翻译中的输入序列和输出序列都是长度可变的。 为了解决这类问题，我们在[4.5](https://github.com/Gary-code/Machine-Learning-Park/tree/main/Part2%20Deep%20Learning%20Practice/4%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/4.5%20%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%8E%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84)小节中 设计了一个通用的”编码器－解码器“架构。 本小节，我们将使用两个循环神经网络的编码器和解码器， 并将其应用于序列到序列（Seq2Seq）类的学习任务。\n",
    "\n",
    "动机：\n",
    "* 给定一个源语言的句子，自动翻译成目标语言\n",
    "* 这两个句子可以有不同的长度"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.6.1 模型架构\n",
    "> Encoder-decoder架构\n",
    "\n",
    "* 编码器是一个RNN，读取输入句子\n",
    "  * 可以是双向（一般双向可以用作encoder，看到整个句子）\n",
    "* 解码器使用另外一个RNN来输出\n",
    "\n",
    "其模型架构图如下图所示：\n",
    "![image-20220210101638258](https://s2.loli.net/2022/02/10/d1SRiOhQYo7U3xr.png)\n",
    "\n",
    "![image-20220210102839313](https://s2.loli.net/2022/02/10/S2l9AthZPLweMRp.png)\n",
    "\n",
    "从上图可以看到:\n",
    "* 编码器是没有输出的RNN\n",
    "* 编码器最后时间步的隐状态用作解码器的初始隐状态\n",
    "\n",
    "因此在**训练**的时候:\n",
    "* 机器翻译的时候\n",
    "    * 训练时解码器使用目标句子作为输出\n",
    "* 推理的时候\n",
    "    * 只能给出上一时刻的输出"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.6.2 衡量生成序列的好坏的BLEU\n",
    "\n",
    "* $p_n$是预测中所有n-gram的精度\n",
    "  * 标签序列ABCDEF和预测序列ABBCD\n",
    "    * $p_1=4/5,p_2=3/4,p_3=1/3,p_4=0$\n",
    "* BLEU定义\n",
    "  * 越大越好！\n",
    "\n",
    "$$\n",
    "\\exp \\left(\\min \\left(0,1-\\frac{1 \\mathrm{en}_{\\text {label }}}{\\operatorname{len}_{\\text {pred }}}\\right) \\prod_{n=1}^{k} p_{n}^{1 / 2^{n}}\\right.\n",
    "$$\n",
    "\n",
    "$\\mathbf{exp(...)}$为惩罚过短的预测,$\\Pi...$表示长匹配有效权重"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.6.3 代码实践\n",
    "\n",
    "使用[4.5](https://github.com/Gary-code/Machine-Learning-Park/tree/main/Part2%20Deep%20Learning%20Practice/4%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/4.5%20%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%8E%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84)小节讲到的机器翻译数据集（英语 -> 法语）进行训练和预测。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 编码器实现"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"Seq2Seq的编码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "        # 编码器不需要输出层\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        \"\"\"\n",
    "        :param self:\n",
    "        :param X: [batch_size, num_steps, vocab_size]\n",
    "        :param args:\n",
    "        :return:\n",
    "        output[num_steps, batch_size, num_hiddens]\n",
    "        state:[num_layers, batch_size, num_hiddens]\n",
    "        \"\"\"\n",
    "        X = self.embedding(X)  # [:, :, vocab_size] -> [:, :, embed_size]\n",
    "        X = X.permute(1, 0, 2)  # [num_steps, batch_size, embed_size]\n",
    "        output, state = self.rnn(X)  # 如果没有提及状态，默认为0\n",
    "        return output, state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试编码器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.eval()\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape, state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 解码器的实现:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"Seq2Seq解码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)  # 输出层\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]  # [output, state] 1就代表state\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.embedding(X).permute(1, 0, 2)  # 变为[num_steps, batch_size, embed_size]\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        print(f'X',X.shape)\n",
    "        print(f'state',state.shape)\n",
    "        print(f'state1',state[-1].repeat(X.shape[0], 1, 1).shape)\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)  # 取出最近的一个隐藏状态\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        print(f'X_and_Context', X_and_context.shape)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state[0]的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试解码器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([7, 4, 8])\n",
      "state torch.Size([2, 4, 16])\n",
      "state1 torch.Size([7, 4, 16])\n",
      "X_and_Context torch.Size([7, 4, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 想通过**0值**屏蔽不相关的项"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask tensor([[ True, False, False],\n",
      "        [ True,  True, False]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[1, 0, 0],\n        [4, 5, 0]])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"\n",
    "    在序列中屏蔽不相关的项\n",
    "    :param X:\n",
    "    :param valid_len:\n",
    "    :param value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "    print(f'mask', mask)\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们还可以使用此函数屏蔽最后几个轴上的所有项。如果愿意，也可以使用指定的非零值来替换这些项。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask tensor([[ True, False, False],\n",
      "        [ True,  True, False]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[ 1.,  1.,  1.,  1.],\n         [-1., -1., -1., -1.],\n         [-1., -1., -1., -1.]],\n\n        [[ 1.,  1.,  1.,  1.],\n         [ 1.,  1.,  1.,  1.],\n         [-1., -1., -1., -1.]]])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, 4)\n",
    "sequence_mask(X, torch.tensor([1, 2]), value=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。注意：填充不参与计算。\n",
    "\n",
    "最初，所有预测词元的掩码都设置为1。 一旦给定了有效长度，填充词元对应的掩码将被设置为0。 最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。\n",
    "\n",
    "**注意:** `unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)` pytorch要求预测的维度需要放在中间"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        \"\"\"\n",
    "        :param pred:  (batch_size,num_steps,vocab_size)\n",
    "        :param label:  (batch_size,num_steps)\n",
    "        :param valid_len: (batch_size,)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)  # 有效的1无效的0\n",
    "        self.reduction='none'  # 不要求mean之类的\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)  # pytorch要求预测的维度需要放在中间\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)  # 对每个句子取一个平均\n",
    "        return weighted_loss  # 对每一个样本（句子）返回一个loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们可以创建三个相同的序列来进行代码健全性检查， 然后分别指定这些序列的有效长度为4、2和0。 结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask tensor([[ True,  True,  True,  True],\n",
      "        [ True,  True, False, False],\n",
      "        [False, False, False, False]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([2.3026, 1.1513, 0.0000])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),\n",
    "     torch.tensor([4, 2, 0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                               device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()      # 损失函数的标量进行“反向传播”\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "          f'tokens/sec on {str(device)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于资源优先这里不跑训练的代码了，读者可自行将`train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)`的注释去掉运行查看训练效果。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "# train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}