{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TIMIT framewise phoneme classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TIMIT framewise phoneme classification\n",
        "[比赛地址](https://www.kaggle.com/c/ml2021spring-hw2)\n",
        "\n",
        "*Attention*:由于这个数据集**较大**，强烈建议读者基于[Colab](https://colab.research.google.com/github/deepmind/mathematics_conjectures/blob/main/knot_theory.ipynb#scrollTo=J5d5Jwqu01qg)或[AI studio](https://aistudio.baidu.com/aistudio/index)等环境运行代码。\n",
        "\n",
        "**这里演示Colab环境下运行代码。**"
      ],
      "metadata": {
        "id": "oNNrfnYNNNDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Environment "
      ],
      "metadata": {
        "id": "3WmddxsnNcMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLgYhAUdNQnD",
        "outputId": "2d2f91f1-1ce1-4a6a-c69d-949eb3ecba36"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04TfPZ3MNlvT",
        "outputId": "7806946f-69bc-439d-876c-1993dee95bde"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download 100%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6dxij6fNnzr",
        "outputId": "bda71931-d009-49eb-ad4b-8c900cc851ba"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "VwITskGDNrLV"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download Data with kaggle API\n",
        "> 记得同意比赛规则，否则会报错无法下载数据。\n",
        "**若出现401报错，可以通过Kaggle重新生成Token文件解决**"
      ],
      "metadata": {
        "id": "t9_D4zONNzkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/dataset/TIMIT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hrEKdjDSKUM",
        "outputId": "5be792cb-78f1-4c5d-acdc-4f20a701b0d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dataset/TIMIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c ml2021spring-hw2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Zkv3rPMOA5T",
        "outputId": "615256b5-b307-4d56-a81f-5f5a9b7d1b24"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading test_11.npy.zip to /content/drive/MyDrive\n",
            " 94% 90.0M/95.9M [00:00<00:00, 102MB/s] \n",
            "100% 95.9M/95.9M [00:00<00:00, 104MB/s]\n",
            "Downloading train_11.npy.zip to /content/drive/MyDrive\n",
            " 96% 252M/261M [00:02<00:00, 132MB/s]\n",
            "100% 261M/261M [00:02<00:00, 107MB/s]\n",
            "Downloading train_label_11.npy.zip to /content/drive/MyDrive\n",
            "  0% 0.00/462k [00:00<?, ?B/s]\n",
            "100% 462k/462k [00:00<00:00, 30.5MB/s]\n",
            "Downloading sampleSubmission.csv.zip to /content/drive/MyDrive\n",
            "  0% 0.00/970k [00:00<?, ?B/s]\n",
            "100% 970k/970k [00:00<00:00, 43.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!pwd\n",
        "!unzip test_11.npy.zip\n",
        "!unzip train_11.npy.zip\n",
        "!unzip train_label_11.npy.zip\n",
        "\n",
        "!rm test_11.npy.zip\n",
        "!rm train_11.npy.zip\n",
        "!rm train_label_11.npy.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwf3yRF9OEto",
        "outputId": "f8c51439-0415-4149-f665-014447c6c870"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_11.npy.zip  train_11.npy.zip  train_label_11.npy.zip\n",
            "/content/drive/MyDrive/dataset/TIMIT\n",
            "Archive:  test_11.npy.zip\n",
            "  inflating: test_11.npy             \n",
            "Archive:  train_11.npy.zip\n",
            "  inflating: train_11.npy            \n",
            "Archive:  train_label_11.npy.zip\n",
            "  inflating: train_label_11.npy      \n",
            "test_11.npy  train_11.npy  train_label_11.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5IU8JHQWhJV",
        "outputId": "f7d0f2fe-5752-4d58-83d9-fe34697518c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_11.npy  train_11.npy  train_label_11.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Dataset and Preprocess"
      ],
      "metadata": {
        "id": "GzS2VBlASVVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print('Loading data ...')\n",
        "\n",
        "data_root='./'\n",
        "train = np.load(data_root + 'train_11.npy')\n",
        "train_label = np.load(data_root + 'train_label_11.npy')\n",
        "test = np.load(data_root + 'test_11.npy')\n",
        "\n",
        "print('Size of training data: {}'.format(train.shape))\n",
        "print('Size of testing data: {}'.format(test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOM1BvOhSqdx",
        "outputId": "5bcfe0ef-b6f1-40ca-d481-1e0269674dfc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data ...\n",
            "Size of training data: (1229932, 429)\n",
            "Size of testing data: (451552, 429)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset"
      ],
      "metadata": {
        "id": "GJc-z9KwTaZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TIMITDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.LongTensor(y)\n",
        "        else:\n",
        "            self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "metadata": {
        "id": "N29FB3p1TdUy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split data into train and valid\n",
        "读者可以通过控制`VAL_RATIO `参数来调整验证集的大小"
      ],
      "metadata": {
        "id": "jyV4pxRRTldL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_RATIO = 0.2\n",
        "\n",
        "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
        "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
        "print('Size of training set: {}'.format(train_x.shape))\n",
        "print('Size of validation set: {}'.format(val_x.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DKT37PcTxPT",
        "outputId": "7c9ba4e0-09bc-41bf-8288-d75ed9c78ab5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training set: (983945, 429)\n",
            "Size of validation set: (245987, 429)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a dataloader\n",
        "> shuffle and batch_size"
      ],
      "metadata": {
        "id": "b61VhO-hT905"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_set = TIMITDataset(train_x, train_y)\n",
        "val_set = TIMITDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True) #only shuffle the training data\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "zd42xmCQUHDC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cleanup the unneeded variables to save memory\n",
        "\n",
        "在kaggle的一些比赛当中，为了*节约我们有限的内存*，保证模型最后训练成功，建议将一些无用的变量即使清除掉，方法如下："
      ],
      "metadata": {
        "id": "pgMrWTuUUVFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del train, train_label, train_x, train_y, val_x, val_y\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99ynZwKQUlRh",
        "outputId": "8f5efa09-db48-44eb-ee1b-0210b3117744"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Model\n"
      ],
      "metadata": {
        "id": "ZuEz1gDWUtKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(429, 1024)\n",
        "        self.layer2 = nn.Linear(1024, 512)\n",
        "        self.layer3 = nn.Linear(512, 128)\n",
        "        self.out = nn.Linear(128, 39) \n",
        "\n",
        "        self.act_fn = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        x = self.act_fn(x)\n",
        "\n",
        "        x = self.out(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "nnpYX7BqUxcM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "v_M7L36jU0nT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check device\n",
        "def get_device():\n",
        "  return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Fix random seeds for reproducibility.\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "LY8NTU4XU2hW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training parameters\n",
        "定义模型的一些超参数"
      ],
      "metadata": {
        "id": "gadO8CGJVGqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "same_seeds(0)\n",
        "\n",
        "# get device \n",
        "device = get_device()\n",
        "print(f'DEVICE: {device}')\n",
        "\n",
        "# training parameters\n",
        "num_epoch = 20               # number of training epoch\n",
        "learning_rate = 0.0001       # learning rate\n",
        "\n",
        "# the path where checkpoint saved\n",
        "model_path = './model.ckpt'\n",
        "\n",
        "# create model, define a loss function, and optimizer\n",
        "model = Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtkG9MTWVJ-R",
        "outputId": "b44d8ac6-8249-4f61-eb75-794aed5e195c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Start training "
      ],
      "metadata": {
        "id": "nX5b__F1VPB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0.0\n",
        "for epoch in range(num_epoch):\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # training\n",
        "    model.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(inputs) \n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        _, train_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "\n",
        "        train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        model.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                _, val_pred = torch.max(outputs, 1) \n",
        "            \n",
        "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
        "            ))\n",
        "\n",
        "            # if the model improves, save a checkpoint at this epoch\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                print('saving model with acc {:.3f}'.format(best_acc/len(val_set)))\n",
        "    else:\n",
        "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
        "            epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(train_loader)\n",
        "        ))\n",
        "\n",
        "# if not validating, save the last epoch\n",
        "if len(val_set) == 0:\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print('saving model at last epoch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxlmXTfRVbEB",
        "outputId": "a9f6460e-1f0f-4f17-b0b4-d0d3a276674c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[001/020] Train Acc: 0.467302 Loss: 1.811661 | Val Acc: 0.567428 loss: 1.433065\n",
            "saving model with acc 0.567\n",
            "[002/020] Train Acc: 0.594383 Loss: 1.330666 | Val Acc: 0.628639 loss: 1.211098\n",
            "saving model with acc 0.629\n",
            "[003/020] Train Acc: 0.644506 Loss: 1.154064 | Val Acc: 0.660421 loss: 1.101216\n",
            "saving model with acc 0.660\n",
            "[004/020] Train Acc: 0.672216 Loss: 1.052246 | Val Acc: 0.676300 loss: 1.038718\n",
            "saving model with acc 0.676\n",
            "[005/020] Train Acc: 0.691347 Loss: 0.983104 | Val Acc: 0.685154 loss: 1.001852\n",
            "saving model with acc 0.685\n",
            "[006/020] Train Acc: 0.705615 Loss: 0.931955 | Val Acc: 0.689301 loss: 0.984177\n",
            "saving model with acc 0.689\n",
            "[007/020] Train Acc: 0.716343 Loss: 0.891687 | Val Acc: 0.694516 loss: 0.964627\n",
            "saving model with acc 0.695\n",
            "[008/020] Train Acc: 0.725881 Loss: 0.857907 | Val Acc: 0.697720 loss: 0.951889\n",
            "saving model with acc 0.698\n",
            "[009/020] Train Acc: 0.733717 Loss: 0.829495 | Val Acc: 0.696691 loss: 0.949866\n",
            "[010/020] Train Acc: 0.741152 Loss: 0.803701 | Val Acc: 0.699374 loss: 0.944832\n",
            "saving model with acc 0.699\n",
            "[011/020] Train Acc: 0.748050 Loss: 0.781106 | Val Acc: 0.697773 loss: 0.946494\n",
            "[012/020] Train Acc: 0.753793 Loss: 0.760380 | Val Acc: 0.702830 loss: 0.938236\n",
            "saving model with acc 0.703\n",
            "[013/020] Train Acc: 0.759404 Loss: 0.741234 | Val Acc: 0.700456 loss: 0.945627\n",
            "[014/020] Train Acc: 0.764573 Loss: 0.723574 | Val Acc: 0.702159 loss: 0.942118\n",
            "[015/020] Train Acc: 0.769471 Loss: 0.707325 | Val Acc: 0.704432 loss: 0.936154\n",
            "saving model with acc 0.704\n",
            "[016/020] Train Acc: 0.773687 Loss: 0.691314 | Val Acc: 0.701736 loss: 0.945713\n",
            "[017/020] Train Acc: 0.778676 Loss: 0.676633 | Val Acc: 0.701586 loss: 0.953081\n",
            "[018/020] Train Acc: 0.783106 Loss: 0.662425 | Val Acc: 0.699671 loss: 0.963290\n",
            "[019/020] Train Acc: 0.786396 Loss: 0.649180 | Val Acc: 0.700086 loss: 0.957681\n",
            "[020/020] Train Acc: 0.790644 Loss: 0.636623 | Val Acc: 0.699736 loss: 0.964269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n",
        "创建一个测试数据集，并从加载模型。\n"
      ],
      "metadata": {
        "id": "uFt_Opv1Vc16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create testing dataset\n",
        "test_set = TIMITDataset(test, None)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# create model and load weights from checkpoint\n",
        "model = Classifier().to(device)\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLAv9gxFVtD4",
        "outputId": "bec4e221-553a-4cb8-fe5d-4e34e84475b1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### predict\n",
        "预测，并将结果保存为一个csv文件"
      ],
      "metadata": {
        "id": "aT_BMcfdVvIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict = []\n",
        "model.eval() # set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        inputs = data\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, test_pred = torch.max(outputs, 1) # get the index of the class with the highest probability\n",
        "\n",
        "        for y in test_pred.cpu().numpy():\n",
        "            predict.append(y)\n",
        "\n",
        "\n",
        "\n",
        "with open('prediction.csv', 'w') as f:\n",
        "    f.write('Id,Class\\n')\n",
        "    for i, y in enumerate(predict):\n",
        "        f.write('{},{}\\n'.format(i, y))"
      ],
      "metadata": {
        "id": "V0gT6wc4VyVW"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}