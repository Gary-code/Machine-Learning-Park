{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.3 训练技巧与实战\n",
    "前面我们对神经网络的基本训练过程都进行了详细的介绍，但实践过程当中，我们通常会遇到很多情况导致我们的神经网络训练到某个程度之后就无法继续优化（前进），如何解决这个问题是2.3所要探讨的重点，我们主要分为：\n",
    "* 局部最小值(local minima)和鞍点(saddle points)\n",
    "* 批次(batch)与动量(momentum)\n",
    "* 自动调整学习率(learning rate)  (*关于学习率的一些优化算法，会在后面的章节进行讲解与实践*)\n",
    "* 损失函数选择(loss function)  (*Part1已做过介绍这里不再讲解*)\n",
    "* 批量标准化(batch normalization)\n",
    "\n",
    "*实战部分*为**两个比赛**：\n",
    "> 训练过程建议读者在[Colab](https://colab.research.google.com/)或者[Kaggle](https://www.kaggle.com/)上选用GPU加速跑模型。\n",
    "* [COVID-19 Cases Prediction](https://www.kaggle.com/c/ml2021spring-hw1)\n",
    "* [TIMIT framewise phoneme classification](https://www.kaggle.com/c/ml2021spring-hw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.1 训练技巧\n",
    "\n",
    "在讲解训练中遇到的问题之前，我们现在对训练的*架构(Framework)*进行一下总览：\n",
    "\n",
    "**Training**\n",
    "Training data: $\\left\\{\\left(x^{1}, \\hat{y}^{1}\\right),\\left(x^{2}, \\hat{y}^{2}\\right), \\ldots,\\left(x^{N}, \\hat{y}^{N}\\right)\\right\\}$\n",
    "\n",
    "训练步骤(Training Steps):\n",
    "* Step 1: 初始化模型参数，$y=f_\\theta(x)$\n",
    "* Step 2: 定义损失函数,$L(\\theta)$\n",
    "* Step 3: 优化，$\\boldsymbol{\\theta}^{*}=\\arg \\min _{\\boldsymbol{\\theta}} L$\n",
    "\n",
    "**Testing**\n",
    "Testing data: $\\left\\{x^{N+1}, x^{N+2}, \\ldots, x^{N+M}\\right\\}$\n",
    "* 使用$f_{\\theta^*}(x)$预测测试集的标签。\n",
    "\n",
    "![](https://s2.loli.net/2022/01/18/cX2tj8ULx6M3SwK.png)\n",
    "\n",
    "\n",
    "**过拟合（Overfitting）**\n",
    "如下图所示，训练集误差减小，但测试集误差很大，往往发生了过拟合的现象：\n",
    "![](https://s2.loli.net/2022/01/18/5sMrJuzXgRk3xH9.png)\n",
    "\n",
    "* 在数据层面上，解决方法就是训练*更多的数据*：\n",
    "![](https://s2.loli.net/2022/01/18/FoBqPgenW7NkIRa.png)\n",
    "* 而在模型层面上的解决方法有:\n",
    "    * 更少的参数，或者共享参数(简化模型)\n",
    "    ![](https://s2.loli.net/2022/01/18/JKjtgfakyQ2C54o.png)\n",
    "    * 更少的特征\n",
    "    * Early Stopping\n",
    "    * 正则化(*Regularization*)\n",
    "    * Dropout\n",
    "    * 一个经典的例子就是CNN(*卷积神经网络*)\n",
    "![](https://s2.loli.net/2022/01/18/v2f6GHNK49lXw5Y.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 权重初始化\n",
    "* **期望为0**\n",
    "* 输入与输出**方差一样**，所以$n_{t-1} \\gamma_{t}=1$。 因为 $h^t$ 是由 $t-1$ 层的 $n$ 个参数 $w$ 运算求得的，而 $t-1$ 层的这些参数之前假设了他们都是服从方差为 $\\gamma$ 的分布，所以他们相加就成了 $n_{t-1}\\gamma$, 了。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Var}\\left[h_{i}^{t}\\right] &=\\mathbb{E}\\left[\\left(h_{i}^{t}\\right)^{2}\\right]-\\mathbb{E}\\left[h_{i}^{t}\\right]^{2}=\\mathbb{E}\\left[\\left(\\sum_{j} w_{i, j}^{t} h_{j}^{t-1}\\right)^{2}\\right] \\\\\n",
    "&=\\mathbb{E}\\left[\\sum_{j}\\left(w_{i, j}^{l}\\right)^{2}\\left(h_{j}^{t-1}\\right)^{2}+\\sum_{j \\neq k} w_{i, j}^{l} w_{i, k}^{t} h_{j}^{t-1} h_{k}^{t-1}\\right] \\\\\n",
    "&=\\sum_{j} \\mathbb{E}\\left[\\left(w_{i, j}^{l}\\right)^{2}\\right] \\mathbb{E}\\left[\\left(h_{j}^{t-1}\\right)^{2}\\right] \\\\\n",
    "&=\\sum_{j} \\operatorname{Var}\\left[w_{i, j}^{t}\\right] \\operatorname{Var}\\left[h_{j}^{t-1}\\right]=n_{t-1} \\gamma_{t} \\operatorname{Var}\\left[h_{j}^{t-1}\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "综上所述，可以推导出：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial \\ell}{\\partial \\mathbf{h}^{t-1}}=\\frac{\\partial \\ell}{\\partial \\mathbf{h}^{t}} \\mathbf{W}^{t} =>\\quad\\left(\\frac{\\partial \\ell}{\\partial \\mathbf{h}^{t-1}}\\right)^{T}=\\left(W^{t}\\right)^{T}\\left(\\frac{\\partial \\ell}{\\partial \\mathbf{h}^{t}}\\right)^{T} \\\\\n",
    "&\\mathbb{E}\\left[\\frac{\\partial \\ell}{\\partial h_{i}^{t-1}}\\right]=0 \\\\\n",
    "&\\operatorname{Var}\\left[\\frac{\\partial \\ell}{\\partial h_{i}^{t-1}}\\right]=n_{t} \\gamma_{t} \\operatorname{Var}\\left[\\frac{\\partial \\ell}{\\partial h_{j}^{t}}\\right] \\quad=>n_{t} \\gamma_{t}=1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "**$Xavier$初始化**\n",
    "实际上，输入输出是很难控制的，难以满足$n_{t-1} \\gamma = 1 和 n_t \\gamma=1$\n",
    "Xavier使得\n",
    "$$\\gamma_{t}\\left(n_{t-1}+n_{t}\\right) / 2=1 \\quad \\rightarrow \\gamma_{t}=2 /\\left(n_{t-1}+n_{t}\\right)$$\n",
    "为了适配权重变化，特别是$n_t$。*正态分布*与*均匀分布*表示如下所示：\n",
    "$$\\begin{aligned}\n",
    "&\\mathcal{N}\\left(0, \\sqrt{2 /\\left(n_{t-1}+n_{t}\\right)}\\right) \\\\\n",
    "&\\mathscr{U}\\left(-\\sqrt{6 /\\left(n_{t-1}+n_{t}\\right)}, \\sqrt{6 /\\left(n_{t-1}+n_{t}\\right)}\\right) \\text { 分布 } \\mathscr{U}[-a, a] \\text { 和方差是 } a^{2} / 3\n",
    "\\end{aligned}$$\n",
    "\n",
    "代码实现如下所示:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Linear(in_features=2, out_features=1, bias=True)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=math.sqrt(2/(m.in_features+m.out_features)))\n",
    "nn.Sequential(nn.Linear(2, 1)).apply(init_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 更小的梯度\n",
    "\n",
    "##### 局部最小值（Local minimal）与鞍点（Saddle point）\n",
    "损失函数在*局部最小值*和*鞍点*的时候，梯度大小都会为0，但两者显著的区别如下图所示:\n",
    "![](https://s2.loli.net/2022/01/18/JqjvaE7N9w841WP.png)\n",
    "\n",
    "我们可以清楚的看到，鞍点的位置我们是有路可走的，但在局部最小值的地方我们会陷入一个“峡谷”当中。换而言之，鞍点情况下进行优化比在局部最小值继续优化更为*简单*。\n",
    "\n",
    "为此我们需要借助数学的工具对这两种情况进行判定, 可见[推理过程](http://www.offconvex.org/2016/03/22/saddlepoints/)\n",
    "\n",
    "实际情况下，通过大量的实验证明，我们的模型会更多的处在鞍点的位置，而并非局部最小值处，因此训练过程中，我们完全可以大胆的进行梯度的调节。\n",
    "\n",
    "![](https://s2.loli.net/2022/01/18/4aZnJARtYj3UsB7.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 批次（Batch）\n",
    "\n",
    "在$\\boldsymbol{\\theta}^{*}=\\arg \\min _{\\boldsymbol{\\theta}} L$过程当中，使用批次训练过程如下：\n",
    "![](https://s2.loli.net/2022/01/18/gnLA5QKtkxHyerl.png)\n",
    "\n",
    "实际上考虑到并行计算的因素，**大的批次**对训练时间是*没有显著的影响*（除非特别大的Batch Size），但**小的批次**运行完一个epoch需要花费*更长的时间*。\n",
    "\n",
    "![](https://s2.loli.net/2022/01/18/TWlGrud1R4MUYAJ.png)\n",
    "\n",
    "在*MNIST*和*CIFAR-10*的两个数据集当中，批次大小与准确度的关系如下所示：\n",
    "![](https://s2.loli.net/2022/01/18/qX24w7KSTyBGQ1n.png)\n",
    "\n",
    "所以Batch Size的合理设置十分重要，下面是关于一些Batch Size大小的对比：\n",
    "\n",
    "| Batch Size           | Small      | Large                |\n",
    "| -------------------- | ---------- | -------------------- |\n",
    "| Speed for one update | Same       | Same (not too large) |\n",
    "| Time for one epoch   | Slower     | **Faster**           |\n",
    "| Gradient             | Noisy      | Stable               |\n",
    "| Optimization         | **Better** | Worse                |\n",
    "| Generalization       | **Better** | Worse                |\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 动量(Momentum)\n",
    "$m^t=\\lambda m^{t-1} - \\eta g^{t-1}$, $m^0=0$\n",
    "使用动量前后对比：\n",
    "* 前:\n",
    "![](https://s2.loli.net/2022/01/18/aTc49M7XvFzASOo.png)\n",
    "* 后\n",
    "![](https://s2.loli.net/2022/01/18/eZUbiRzYgIxCo9s.png)\n",
    "\n",
    "\n",
    "在实际例子当中，动量可以让我们更容易跳出局部最小值，使得模型可以继续优化下去:\n",
    "![](https://s2.loli.net/2022/01/18/vx4gGSVd9uYIrnC.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 批量标准化(*BN*)\n",
    "> 几乎所有卷积神经网络都会使用**批量归一化**, 当然另外还有[layer norm](https://arxiv.org/abs/1607.06450)\n",
    "\n",
    "##### 原理\n",
    "仅仅对原始输入数据进行标准化是不充分的，因为虽然这种做法可以保证原始输入数据的质量，但它却无法保证隐藏层输入数据的质量。浅层参数的微弱变化经过多层线性变换与激活函数后被放大，改变了每一层的输入分布，造成深层的网络需要不断调整以适应这些分布变化，最终导致模型**难以训练收敛**。\n",
    "\n",
    "简单的将每层得到的数据进行直接的标准化操作显然是不可行的，因为这样会*破坏每层自身学到的数据特征*。为了使“规范化”之后不破坏层结构本身学到的特征，BN引入了**两个**可以学习的“重构参数”以期望能够从规范化的数据中重构出层本身学到的特征。\n",
    "\n",
    "下面$\\gamma$为方差，$\\beta$为均值\n",
    "\n",
    "* 计算批处理数据均值\n",
    "$$\\mu_{B}=\\frac{1}{|B|} \\sum_{i \\in B} x_{i}$$\n",
    "* 计算批处理数据方差\n",
    "$$\\sigma_{B}^{2}=\\frac{1}{|B|} \\sum_{i \\in B}\\left(x_{i}-\\mu_{B}\\right)^{2}+\\epsilon$$\n",
    "* 规范化\n",
    "$$x_{i+1}=\\gamma \\frac{x_{i}-\\mu_{B}}{\\sigma_{B}}+\\beta$$\n",
    "\n",
    "\n",
    "##### 使用\n",
    "作用在：\n",
    "* 全连接层和卷积层输出上，**激活函数前**\n",
    "* 全连接层和卷积层输入上\n",
    "* 对于全连接层，作用在*特征*维\n",
    "* 对于卷积层，作用在*通道*维\n",
    "\n",
    "使用BN后，可以:\n",
    "* 缓解梯度消失，加速网络收敛。\n",
    "* 简化调参，网络更稳定。BN层抑制了参数微小变化随网络加深而被放大的问题，对参数变化的适应能力更强，更容易调参。\n",
    "* 防止过拟合。BN层将每一个batch的均值和方差引入到网络中，由于每个batch的这俩个值都不相同，可看做为训练过程增 加了随机噪声，可以起到一定的正则效果，防止过拟合。\n",
    "* 另外后续有论文指出它可能就是通过加入噪音来控制模型复杂度，因此**没必要**与`Dropout`混合使用\n",
    "\n",
    "下面我们通过代码实现：\n",
    "* 手动从零开始实现"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    \"\"\"\n",
    "    :param X:\n",
    "    :param gamma:\n",
    "    :param beta:\n",
    "    :param moving_mean: 全局均值\n",
    "    :param moving_var: 权值方差\n",
    "    :param eps:\n",
    "    :param momentum:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not torch.is_grad_enabled():\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)  # 2 为全连接， 4 为卷积\n",
    "        if len(X.shape) == 2:\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 2d卷积\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)  # (0：批量大小，1输入输出通道， 2高， 3宽)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)  # 4D\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 全局更新\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta\n",
    "    return Y, moving_mean.data, moving_var.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "撇开算法细节，注意我们实现层的基础设计模式。 通常情况下，我们用一个单独的函数定义其数学原理，比如说batch_norm。 然后，我们将此功能集成到一个自定义层中，其代码主要处理：\n",
    "1. 数据移动到训练设备（如GPU）\n",
    "2. 分配和初始化任何必需的变量\n",
    "3. 跟踪移动平均线（此处为均值和方差）等问题。\n",
    "为了方便起见，我们并不担心在这里自动推断输入形状，因此我们需要指定整个特征的数量。\n",
    "\n",
    "创建一个BatchNorm层:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features,  1, 1)\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_mean = torch.zeros(shape)  # 这两个参数不用迭代，就放入parameter了\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(X, self.gamma,\n",
    "                                                          self.beta,\n",
    "                                                          self.moving_mean,\n",
    "                                                          self.moving_var,\n",
    "                                                          eps=1e-5,   # 常见设定\n",
    "                                                          momentum=0.9)\n",
    "        return Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "为了简单起见，我们将BN层作用在LeNet上:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "net1 = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Pytorch API实现\n",
    "1. 对2d或3d数据进行批标准化（Batch Normlization）操作：\n",
    "`class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True)`\n",
    "`num_features`：特征的维度 (N,L) -> L ;(N,C,L) -> C\n",
    "2.对由3d数据组成的4d数据（N,C,X,Y）进行Batch Normlization：\n",
    "`class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)`\n",
    "`num_features`：特征的维度 (N,C,X,Y) -> C"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "net2 = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 6, 24, 24])\n",
      "BatchNorm2d output shape:\t torch.Size([1, 6, 24, 24])\n",
      "Sigmoid output shape:\t torch.Size([1, 6, 24, 24])\n",
      "AvgPool2d output shape:\t torch.Size([1, 6, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 16, 8, 8])\n",
      "BatchNorm2d output shape:\t torch.Size([1, 16, 8, 8])\n",
      "Sigmoid output shape:\t torch.Size([1, 16, 8, 8])\n",
      "AvgPool2d output shape:\t torch.Size([1, 16, 4, 4])\n",
      "Flatten output shape:\t torch.Size([1, 256])\n",
      "Linear output shape:\t torch.Size([1, 120])\n",
      "BatchNorm1d output shape:\t torch.Size([1, 120])\n",
      "Sigmoid output shape:\t torch.Size([1, 120])\n",
      "Linear output shape:\t torch.Size([1, 84])\n",
      "BatchNorm1d output shape:\t torch.Size([1, 84])\n",
      "Sigmoid output shape:\t torch.Size([1, 84])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 28, 28)\n",
    "net2.eval()\n",
    "for layer in net2:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3.2 比赛\n",
    "* [COVID-19 Cases Prediction](https://www.kaggle.com/c/ml2021spring-hw1)\n",
    "* [TIMIT framewise phoneme classification](https://www.kaggle.com/c/ml2021spring-hw2)\n",
    "\n",
    "下面代码仅仅展示一个*最基础*的Baseline 代码：\n",
    "* [比赛1](./covid19_prediction.ipynb)\n",
    "* [比赛2](./phoneme_classification.ipynb)\n",
    "\n",
    "并且下面将展示：\n",
    "* 本地Windows环境（比赛1）下  （*由于这个数据集较小才采用这种方式，否则建议使用kaggle或者colab来跑程序*）\n",
    "* Colab环境（比赛2）下如何下载数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "首先安装kaggle官方库"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "登录[Kaggle个人信息版块](https://www.kaggle.com/)，点击“Create New API Token”下载kaagle.json文件:\n",
    "![](https://s2.loli.net/2022/01/18/cF2IbDE7LKvUnkd.png)\n",
    "\n",
    "随后新建'.kaggle'文件夹，将下载的json文件放入，并且整个文件夹移到C盘User目录下即可，最终如下所示:\n",
    "![](https://s2.loli.net/2022/01/18/BHCn35UZ8sWEXAf.png)\n",
    "\n",
    "**最后导入，kaggle包即可**\n",
    "\n",
    "*注意*：用下面命令下载数据前，请务必先*同意该场比赛的规则*:\n",
    "![](https://s2.loli.net/2022/01/18/HsbuUOEme6BK3o4.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}