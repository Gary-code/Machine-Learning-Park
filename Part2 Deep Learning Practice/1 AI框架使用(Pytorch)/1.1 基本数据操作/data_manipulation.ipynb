{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1.1 基本数据操作\n",
    "由于本人能力有限，不可能将所有Pytorch的操作都进行讲解。因此强烈建议读者遇到问题时候查阅Pytorch的[官方文档](https://pytorch.org/docs/stable/index.html) 和参与一些论坛社区的讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 安装\n",
    "对Pytorch的安装，这里也不做过多的展开介绍。可以来看[沐神的视频](https://www.bilibili.com/video/BV18p4y1h7Dr?spm_id_from=333.999.0.0) 来进行学习。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 张量与基本运算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "为此我们首先导入torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 为了后续方便我顺便将下面这些库也导入\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensor的创建"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以通过我们熟悉的List或者Numpy来进行创建"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 1, -1],\n         [ 2, -2]]), tensor([[ 1, -1],\n         [ 2, -2]], dtype=torch.int32))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_form = [[1, -1], [2, -2]]\n",
    "x1 = torch.tensor(list_form)  # 从list中创建\n",
    "x2 = torch.from_numpy(np.array(list_form))  # 从numpy中创建\n",
    "x1, x2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当然tensor也可以转换为numpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1, -1],\n       [ 2, -2]], dtype=int64)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x1.numpy()\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "其他类型tensor的创建\n",
    "1. arange来进行创建"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([12]), 12)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "print(x)\n",
    "x.shape, x.numel()  # 形状,数量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 空Tensor（size为$3 \\times 4$）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(3, 4)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 随机初始化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.9462, 0.6851, 0.3648, 0.6865],\n        [0.5536, 0.1279, 0.8452, 0.2953],\n        [0.0712, 0.8674, 0.8243, 0.8145]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3, 4)  # 元素在(0, 1)之间\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 单位tensor(元素全为1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(3, 4)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. 指定元素类型的tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1, 1, 1, 1],\n         [1, 1, 1, 1],\n         [1, 1, 1, 1]]), torch.int64)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(3, 4, dtype=torch.long)  # 指定long类型\n",
    "x, x.dtype"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. 借助现有tensor创建tensor\n",
    "此方法会默认重用输入Tensor的一些属性，如数据类型等"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.4871,  1.0495,  1.0366,  0.7771],\n        [-0.2144, -0.5354, -1.0920, -0.9977],\n        [-0.8236,  1.5853, -1.1083, -0.2541]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)  # 正态分布，size与x一致\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(3, 4, dtype=torch.float)  # size为(3, 4)的单位tensor\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 基本运算操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 简单四则运算，这里以加法为例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.8645, 1.6269, 1.6203, 1.2528],\n        [1.7455, 1.8965, 1.0428, 1.9916],\n        [1.9243, 1.0609, 1.3821, 1.4581]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(3, 4, dtype=torch.float)\n",
    "y = torch.rand(3, 4)\n",
    "x + y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.8645, 1.6269, 1.6203, 1.2528],\n        [1.7455, 1.8965, 1.0428, 1.9916],\n        [1.9243, 1.0609, 1.3821, 1.4581]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.add(x, y)\n",
    "z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "add_代表inplace版本。pytorch其他函数也类似如x.copy_(y), x.t_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)\n",
    "y == z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 索引与形状"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x[0, :]\n",
    "y += 1\n",
    "y == x[0, :]  # 结果为True。证明源tensor也会改变"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "view和reshape是常用的改变tensor.shape的函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3, 4]), torch.Size([12]), torch.Size([2, 6]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(12)\n",
    "z = x.view(-1, 6)  # -1所指的维度可以根据其他维度的值推出来\n",
    "x.size(), y.size(), z.size()  # x.size开始时候为(3, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "深拷贝"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[3., 3., 3., 3.],\n         [2., 2., 2., 2.],\n         [2., 2., 2., 2.]]),\n tensor([3., 3., 3., 3., 2., 2., 2., 2., 2., 2., 2., 2.]))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "x, y  # True, y的值也会跟着改变, 即使他们的shape不同。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "因此如果我们想得到一个真正的副本而不是像上边那样共享内存，可以考虑使用reshape()函数。还有另外一个解决方案就是使用clone创建一个副本再使用view"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[2., 2., 2., 2.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]),\n tensor([3., 3., 3., 3., 2., 2., 2., 2., 2., 2., 2., 2.]))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cp = x.clone().view(12)\n",
    "x -= 1\n",
    "x, x_cp  # x_cp不会跟着x改变"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Squeeze/Unsqueeze去除(增加)长度为1的指定维度(具体更多的参数可以看官方文档)\n",
    "\n",
    "* squeeze 去除\n",
    "![](https://s2.loli.net/2022/01/10/ifH7OJDpz4CQndv.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "former shape: torch.Size([1, 2, 3])\n",
      "shape after squeeze: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([1, 2, 3])\n",
    "print(f'former shape:', x.shape)  # (1, 2, 3)\n",
    "x.squeeze_(0)  # 可以指定维度，也可以不指定\n",
    "print(f'shape after squeeze:', x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 增加\n",
    "![](https://s2.loli.net/2022/01/10/KeJY9iPyaCEu3WQ.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "former shape: torch.Size([2, 3])\n",
      "shape after unsqueeze: torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([2, 3])\n",
    "print(f'former shape:', x.shape)\n",
    "x = x.unsqueeze(1)  # 在维度为1处添加\n",
    "print(f'shape after unsqueeze:', x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 张量转置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 2])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros([2, 3])\n",
    "x = x.transpose(0, 1)  # 转置的维度\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. 连接多个tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 6, 3])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros([2, 1, 3])\n",
    "y = torch.zeros([2, 2, 3])\n",
    "z = torch.zeros([2, 3, 3])\n",
    "a = torch.cat([x, y, z], dim=1)  # 根据维度1来进行连接\n",
    "a.shape  # (2, 6, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 广播机制(Broadcasting) 和内存问题"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 广播机制\n",
    "即先适当复制元素使这两个Tensor形状相同后再按元素运算。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([[1],\n         [2],\n         [3]]), tensor([[2, 3],\n         [3, 4],\n         [4, 5]]))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "y, x + y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 内存问题\n",
    "使用pytorch自带的id函数:\n",
    "* 如果两个实例的ID一致，那么它们所对应的内存地址相同\n",
    "* 反之则不同"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y = y + x\n",
    "id(y) == id_before  # False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如果想指定结果到原来的y的内存，我们可以使用前面介绍的索引来进行替换操作。\n",
    "\n",
    "我们把x + y的结果通过[:]写进y对应的内存中"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "y[:] = y + x  # 仅仅改写元素\n",
    "id(y) == id_before # True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "还可以使用运算符全名函数中的out参数或者自加运算符+=(也即add_())达到上述效果,如:\n",
    "* ```torch.add(x, y, out=y)```\n",
    "* ```y.add_(x)```\n",
    "* ```y += x```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "id_before = id(y)\n",
    "torch.add(x, y, out=y) # y += x, y.add_(x)  # 仅仅改写元素\n",
    "id(y) == id_before # True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "需要注意的是，虽然view返回的Tensor与源Tensor是**共享data的**，但是依然是一个新的Tensor（因为Tensor除了包含data外还有一些其他属性），二者**id（内存地址）并不一致**。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### tensor的运算(利用广播机制)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 累计求和，特别注意axis参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "b: tensor([40, 45, 50, 55])\n",
      "c: tensor([[40, 45, 50, 55]])\n",
      "a/c: tensor([[0.0000, 0.0222, 0.0400, 0.0545],\n",
      "        [0.1000, 0.1111, 0.1200, 0.1273],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.3000, 0.2889, 0.2800, 0.2727],\n",
      "        [0.4000, 0.3778, 0.3600, 0.3455]])\n",
      "a累加求和: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  6,  8, 10],\n",
      "        [12, 15, 18, 21],\n",
      "        [24, 28, 32, 36],\n",
      "        [40, 45, 50, 55]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(20).reshape(5, 4)\n",
    "print(f'a:', a)\n",
    "b = a.sum(axis=0)\n",
    "print(f'b:', b)\n",
    "c = a.sum(axis=0, keepdim=True)  # 可以用广播机制，保留那个求和的维度\n",
    "print(f'c:', c)\n",
    "print(f'a/c:', a/c)\n",
    "\n",
    "# 累加求和\n",
    "print(f'a累加求和:', a.cumsum(axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 矩阵乘法\n",
    "* 矩阵乘向量 ```mv```函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.0371, 0.6458, 0.0891, 0.4870],\n         [0.5749, 0.4237, 0.6312, 0.2870],\n         [0.8863, 0.0082, 0.1821, 0.6658],\n         [0.9929, 0.8793, 0.7258, 0.8459],\n         [0.1341, 0.8832, 0.6228, 0.3695]]),\n tensor([0.5416, 0.5829, 0.9599, 0.5160]),\n torch.Size([5, 4]),\n torch.Size([4]),\n tensor([0.7333, 1.3123, 1.0032, 2.1835, 1.3759]))"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(5, 4)\n",
    "x = torch.rand(4)\n",
    "A, x, A.shape, x.shape, torch.mv(A, x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 矩阵相乘  ```mm```函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.2590, 1.2590, 1.2590],\n        [1.9168, 1.9168, 1.9168],\n        [1.7425, 1.7425, 1.7425],\n        [3.4439, 3.4439, 3.4439],\n        [2.0096, 2.0096, 2.0096]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 范数\n",
    "* $l2$范数\n",
    "$\\|x\\|_{2}=\\left(\\left|x_{1}\\right|^{2}+\\left|x_{2}\\right|^{2}+\\left|x_{3}\\right|^{2}+\\cdots+\\left|x_{n}\\right|^{2}\\right)^{1 / 2}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(5.)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3., -4.])\n",
    "torch.norm(u)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* $l1$范数\n",
    "$|| x||_{1}=\\left|x_{1}\\right|+\\left|x_{2}\\right|+\\left|x_{3}\\right|+\\cdots+\\left|x_{n}\\right|$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(7.)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3., -4.])\n",
    "torch.abs(u).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 矩阵$Frobenius$范数($F$范数，即元素平方和开根)\n",
    "$\\|X\\|_{F} \\stackrel{\\text { def }}{=} \\sqrt{\\sum_{i} \\sum_{j} X_{i, j}^{2}}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(6.)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones(4, 9))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 其他操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 将tensor存放在GPU当中\n",
    "首先，你需要确保你的Win/Linux机器拥有英伟达(NVIDIA)的显卡。[cuda的安装地址](https://developer.nvidia.cn/zh-cn/cuda-toolkit)\n",
    "\n",
    "在后面章节的模型训练中，不要频繁出现tensor在gpu和cpu之间跳转，否则训练时间会大大增加。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  # 查看是否有cuda的设备\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()将tensor转移回去cpu,同时可以更改数据类型。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}