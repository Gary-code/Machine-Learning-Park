{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NBME-Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🤖 BERT baseline for beginners with Pytorch\n",
        "\n",
        "[Competition]()\n",
        "\n",
        "**Note that this training notebook is just a baseline model, which is designed for the beginner. The main aim is helping the NLP beginner to review pytorch and steps of fine tuning. If you want to get a higher rank in the competition, you can do more based on this basic tutorial.**\n",
        "\n",
        "**some information**:\n",
        "* Framework: Pytorch\n",
        "* Model Architecture:\n",
        "  * BERT -> Linear(768, 512) -> Linear(512, 512) -> Linear(512, 1)\n",
        "* learning rate: 1e-5\n",
        "* Batch Size: 8\n",
        "* Epoch: 3\n",
        "* Dropout: 0.2\n",
        "* Criterion: BCEWithLogitsLoss\n",
        "* Optimizer: Adam\n",
        "\n",
        "\n",
        "## Tokenizer parameters\n",
        "* **Max Lenght**: 416\n",
        "* **Padding**: max_lenght\n",
        "* **Truncation**: only_scond"
      ],
      "metadata": {
        "id": "UvuzAP3Aklxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset\n",
        "Before that you should download the dataset file first. The detail steps of downloading can be found in this [notebook](https://github.com/Gary-code/Machine-Learning-Park/blob/main/Part3%20Industrial%20application/NBME%20-%20Score%20Clinical%20Patient%20Notes/NBME_Score_Clinical_Patient_Notes_EDA.ipynb) "
      ],
      "metadata": {
        "id": "KP2lnrmpjVca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/dataset/NBMA\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BepNGNIjoEX",
        "outputId": "f76cfb5d-003e-450d-ca17-e95ae8c945ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dataset/NBMA\n",
            "features.csv  patient_notes.csv.zip  test.csv\n",
            "kaggle.json   sample_submission.csv  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0cMIFMuQPFY",
        "outputId": "5ce548f2-a102-4f6e-f220-1bb1fbef684f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 52.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 31.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "from itertools import chain\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import gc"
      ],
      "metadata": {
        "id": "Mek49DILRJsu"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Useful Functions\n",
        "### Dataset\n",
        "merge `features.csv`, `patient_notes.csv` with `train.csv`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8PNVaRJBkCL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_URL = \".\"\n",
        "\n",
        "def process_feature_text(text):\n",
        "  return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n",
        "\n",
        "\n",
        "def loading_datasets():\n",
        "  features = pd.read_csv(f\"{BASE_URL}/features.csv\")\n",
        "  notes = pd.read_csv(f\"{BASE_URL}/patient_notes.csv.zip\")\n",
        "  df = pd.read_csv(f\"{BASE_URL}/train.csv\")\n",
        "  df[\"annotation_list\"] = [literal_eval(x) for x in df[\"annotation\"]]\n",
        "  df[\"location_list\"] = [literal_eval(x) for x in df[\"location\"]]  # extraction and convert string to list\n",
        "\n",
        "  merged = df.merge(notes, how=\"left\")\n",
        "  merged = merged.merge(features, how=\"left\")\n",
        "\n",
        "  merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n",
        "  merged[\"feature_text\"] = merged[\"feature_text\"].apply(lambda x: x.lower())  #  Convert to lowercase, same as below\n",
        "  merged[\"pn_history\"] = merged[\"pn_history\"].apply(lambda x: x.lower())\n",
        "\n",
        "  return merged"
      ],
      "metadata": {
        "id": "UPRI6A4NREDs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = loading_datasets()\n",
        "tmp.head()  # preview the data after merge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "0KAeK4TgY54M",
        "outputId": "a7b5263d-5d94-4faf-9d70-c422a0ea7eb2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id  case_num  pn_num  feature_num  \\\n",
              "0  00016_000         0      16            0   \n",
              "1  00016_001         0      16            1   \n",
              "2  00016_002         0      16            2   \n",
              "3  00016_003         0      16            3   \n",
              "4  00016_004         0      16            4   \n",
              "\n",
              "                                 annotation              location  \\\n",
              "0          ['dad with recent heart attcak']           ['696 724']   \n",
              "1             ['mom with \"thyroid disease']           ['668 693']   \n",
              "2                        ['chest pressure']           ['203 217']   \n",
              "3      ['intermittent episodes', 'episode']  ['70 91', '176 183']   \n",
              "4  ['felt as if he were going to pass out']           ['222 258']   \n",
              "\n",
              "                          annotation_list     location_list  \\\n",
              "0          [dad with recent heart attcak]         [696 724]   \n",
              "1             [mom with \"thyroid disease]         [668 693]   \n",
              "2                        [chest pressure]         [203 217]   \n",
              "3        [intermittent episodes, episode]  [70 91, 176 183]   \n",
              "4  [felt as if he were going to pass out]         [222 258]   \n",
              "\n",
              "                                          pn_history  \\\n",
              "0  hpi: 17yo m presents with palpitations. patien...   \n",
              "1  hpi: 17yo m presents with palpitations. patien...   \n",
              "2  hpi: 17yo m presents with palpitations. patien...   \n",
              "3  hpi: 17yo m presents with palpitations. patien...   \n",
              "4  hpi: 17yo m presents with palpitations. patien...   \n",
              "\n",
              "                                        feature_text  \n",
              "0  family history of mi; family history of myocar...  \n",
              "1                 family history of thyroid disorder  \n",
              "2                                     chest pressure  \n",
              "3                              intermittent symptoms  \n",
              "4                                        lightheaded  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d4f3d42b-e4c8-486f-99b9-92013b3287c5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>case_num</th>\n",
              "      <th>pn_num</th>\n",
              "      <th>feature_num</th>\n",
              "      <th>annotation</th>\n",
              "      <th>location</th>\n",
              "      <th>annotation_list</th>\n",
              "      <th>location_list</th>\n",
              "      <th>pn_history</th>\n",
              "      <th>feature_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00016_000</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>['dad with recent heart attcak']</td>\n",
              "      <td>['696 724']</td>\n",
              "      <td>[dad with recent heart attcak]</td>\n",
              "      <td>[696 724]</td>\n",
              "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
              "      <td>family history of mi; family history of myocar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00016_001</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>['mom with \"thyroid disease']</td>\n",
              "      <td>['668 693']</td>\n",
              "      <td>[mom with \"thyroid disease]</td>\n",
              "      <td>[668 693]</td>\n",
              "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
              "      <td>family history of thyroid disorder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00016_002</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>2</td>\n",
              "      <td>['chest pressure']</td>\n",
              "      <td>['203 217']</td>\n",
              "      <td>[chest pressure]</td>\n",
              "      <td>[203 217]</td>\n",
              "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
              "      <td>chest pressure</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00016_003</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>['intermittent episodes', 'episode']</td>\n",
              "      <td>['70 91', '176 183']</td>\n",
              "      <td>[intermittent episodes, episode]</td>\n",
              "      <td>[70 91, 176 183]</td>\n",
              "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
              "      <td>intermittent symptoms</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00016_004</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>4</td>\n",
              "      <td>['felt as if he were going to pass out']</td>\n",
              "      <td>['222 258']</td>\n",
              "      <td>[felt as if he were going to pass out]</td>\n",
              "      <td>[222 258]</td>\n",
              "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
              "      <td>lightheaded</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4f3d42b-e4c8-486f-99b9-92013b3287c5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d4f3d42b-e4c8-486f-99b9-92013b3287c5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d4f3d42b-e4c8-486f-99b9-92013b3287c5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp.location_list[3], tmp.location[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c_fEtIcZfd7",
        "outputId": "fb0d1c00-68b6-4338-fbf5-e73e606d792b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['70 91', '176 183'], \"['70 91', '176 183']\")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del tmp\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOSQYCsNZFmz",
        "outputId": "6516d627-d962-4c52-a377-6b9e63d80087"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "640"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "\n",
        "* [Some explaination of params(Chinese)](https://zhuanlan.zhihu.com/p/341994096)\n",
        "* ❤ [Some **Important** details (Chinese)](https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/#%E9%82%A3%E4%B9%88%E9%A2%84%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%99%A8%E9%97%AE%E7%AD%94%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E8%B6%85%E9%95%BF%E6%96%87%E6%9C%AC%E7%9A%84%E5%91%A2)"
      ],
      "metadata": {
        "id": "ngXoQGHeXiiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loc_list_to_ints(loc_list):\n",
        "  \"\"\"location_list -> index\"\"\"\n",
        "  to_return = []\n",
        "  for loc_str in loc_list:\n",
        "    loc_strs = loc_str.split(\";\")\n",
        "    for loc in loc_strs:\n",
        "      start, end = loc.split()\n",
        "      to_return.append((int(start), int(end)))  # add a tuple\n",
        "  return to_return"
      ],
      "metadata": {
        "id": "upHZvz-0Xty6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_add_labels(tokenizer, data, config):\n",
        "  out = tokenizer(\n",
        "      data[\"feature_text\"],\n",
        "        data[\"pn_history\"],\n",
        "        truncation=config['truncation'],\n",
        "        max_length=config['max_length'],\n",
        "        padding=config['padding'],\n",
        "        return_offsets_mapping=config['return_offsets_mapping']\n",
        "  )\n",
        "  labels = [0.0] * len(out[\"input_ids\"])  # input_ids: mapping indices\n",
        "  out[\"location_int\"] = loc_list_to_ints(data[\"location_list\"])\n",
        "  out[\"sequence_ids\"] = out.sequence_ids()  # seperate the first and second param\n",
        "\n",
        "  for idx, (seq_id, offsets) in enumerate(zip(out[\"sequence_ids\"], out[\"offset_mapping\"])):\n",
        "    if not seq_id or seq_id == 0:\n",
        "            labels[idx] = -1\n",
        "            continue\n",
        "    token_start, token_end = offsets\n",
        "    for feature_start, feature_end in out[\"location_int\"]:\n",
        "      if token_start >= feature_start and token_end <= feature_end:\n",
        "        labels[idx] = 1.0\n",
        "        break\n",
        "\n",
        "  out[\"labels\"] = labels\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "OTjgBPF5atCD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predection and Score Function\n",
        "This part is just designed as the description of overview, I will not explain it in details."
      ],
      "metadata": {
        "id": "KASF9EQZbOtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n",
        "    all_predictions = []\n",
        "    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n",
        "        pred = 1 / (1 + np.exp(-pred))\n",
        "        start_idx = None\n",
        "        end_idx = None\n",
        "        current_preds = []\n",
        "        for pred, offset, seq_id in zip(pred, offsets, seq_ids):\n",
        "            if seq_id is None or seq_id == 0:\n",
        "                continue\n",
        "\n",
        "            if pred > 0.5:\n",
        "                if start_idx is None:\n",
        "                    start_idx = offset[0]\n",
        "                end_idx = offset[1]\n",
        "            elif start_idx is not None:\n",
        "                if test:\n",
        "                    current_preds.append(f\"{start_idx} {end_idx}\")\n",
        "                else:\n",
        "                    current_preds.append((start_idx, end_idx))\n",
        "                start_idx = None\n",
        "        if test:\n",
        "            all_predictions.append(\"; \".join(current_preds))\n",
        "        else:\n",
        "            all_predictions.append(current_preds)\n",
        "            \n",
        "    return all_predictions\n",
        "\n",
        "\n",
        "def calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n",
        "\n",
        "        num_chars = max(list(chain(*offsets)))\n",
        "        char_labels = np.zeros(num_chars)\n",
        "\n",
        "        for o, s_id, label in zip(offsets, seq_ids, labels):\n",
        "            if s_id is None or s_id == 0:\n",
        "                continue\n",
        "            if int(label) == 1:\n",
        "                char_labels[o[0]: o[1]] = 1\n",
        "\n",
        "        char_preds = np.zeros(num_chars)\n",
        "\n",
        "        for start_idx, end_idx in preds:\n",
        "            char_preds[start_idx: end_idx] = 1\n",
        "\n",
        "        all_labels.extend(char_labels)\n",
        "        all_preds.extend(char_preds)\n",
        "\n",
        "    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    \n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"precision\": results[0],\n",
        "        \"recall\": results[1],\n",
        "        \"f1\": results[2]\n",
        "    }"
      ],
      "metadata": {
        "id": "ksRf8_F9cYmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construct Dataset"
      ],
      "metadata": {
        "id": "K6mI4fGRmkqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer, config):\n",
        "    super().__init__()\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.config = config\n",
        "  \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    data = self.data.iloc[idx]\n",
        "    tokens = tokenize_with_add_labels(self.tokenizer, data, self.config)\n",
        "\n",
        "    input_ids = np.array(tokens[\"input_ids\"])\n",
        "    attention_mask = np.array(tokens[\"attention_mask\"])\n",
        "    token_type_ids = np.array(tokens[\"token_type_ids\"])\n",
        "\n",
        "    labels = np.array(tokens[\"labels\"])\n",
        "    offset_mapping = np.array(tokens[\"offset_mapping\"])\n",
        "    sequence_ids = np.array(tokens[\"sequence_ids\"]).astype('float16')\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, labels, offset_mapping, sequence_ids  "
      ],
      "metadata": {
        "id": "xJYbZec5mnv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "* **BERT** base\n",
        "* with some FC layers"
      ],
      "metadata": {
        "id": "FgGqUXO5oX-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Moudle):\n",
        "  def __init__(self, config):\n",
        "    super().__init__\n",
        "    self.bert = AutoModel.from_pretrained(config['model_name'])  # BERT\n",
        "    self.dropout = nn.Dropout(p=config['dropout'])\n",
        "    self.config = config\n",
        "    self.fc1 = nn.Linear(768, 512)\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.fc3 = nn.Linear(256, 1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "    logits = self.fc1(outputs[0])\n",
        "    logits = self.fc2(self.dropout(logits))\n",
        "    logits = self.fc3(self.dropout(logits)).squeeze(-1)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Cm96Qkoyofro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ],
      "metadata": {
        "id": "5C2D6BZ1pqOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    \"max_length\": 416,\n",
        "    \"padding\": \"max_length\",\n",
        "    \"return_offsets_mapping\": True,\n",
        "    \"truncation\": \"only_second\",\n",
        "    \"model_name\": \"bert-base-uncased\",\n",
        "    \"dropout\": 0.2,\n",
        "    \"lr\": 1e-5,\n",
        "    \"test_size\": 0.2,\n",
        "    \"seed\": 1268,\n",
        "    \"batch_size\": 8\n",
        "}"
      ],
      "metadata": {
        "id": "D3JX3lDMptG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training(fine tuning)\n",
        "\n",
        "### Prepare Dataset\n",
        "\n",
        "* Train and Test split: 20%\n",
        "  * Train: 11440\n",
        "  * Test: 2860\n"
      ],
      "metadata": {
        "id": "eFOY35IUquAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = loading_datasets()\n",
        "X_train, X_test = train_test_split(train_df, test_size=hyperparameters['test_size'], random_state=hyperparameters['seed'])\n",
        "\n",
        "print(f\"Train Size:\", len(X_train))\n",
        "print(f\"Test Size:\", len(X_test))"
      ],
      "metadata": {
        "id": "MeYDFW42q7G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(hyperparameters['model_name'])\n",
        "\n",
        "training_data = MyDataset(X_train, tokenizer, hyperparameters)\n",
        "train_dataloader = DataLoader(training_data, batch_size=hyperparameters['batch_size'], shuffle=True)\n",
        "\n",
        "testing_data = MyDataset(X_test, tokenizer, hyperparameters)\n",
        "test_dataloader = DataLoader(testing_data, batch_size=hyperparameters['batch_size'], shuffle=False)"
      ],
      "metadata": {
        "id": "5dStwdbarYEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start Training\n",
        "Lets train the model with **BCEWithLogitsLoss** and **AdamW** as optimizer.\n",
        "\n",
        "⚒ **Notes that:** on BCEWithLogitsLoss, the default value for reduction is `mean` (the sum of the output will be divided by the number of elements in the output). If we use this default value, it will produce negative loss. Because we have some negative labels. To fix this negative loss issue, we can use `none` as parameter. To calculate the mean, first, we have to filter out the negative values. [DOC](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T7GXdvWlsMHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = MyModel(hyperparameters).to(device)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss(reduce = \"none\")\n",
        "optimizer = optim.AdamW(model.parameters(), lr=hyperparameters['lr'])"
      ],
      "metadata": {
        "id": "TVgu9dERslZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚴 [torch.masked_select](https://zhuanlan.zhihu.com/p/348035584)"
      ],
      "metadata": {
        "id": "H7Pg_qNzvj37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "  model.train()\n",
        "  train_loss = []\n",
        "\n",
        "  for batch in tqdm(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_mask = batch[1].to(device)\n",
        "    token_type_ids = batch[2].to(device)\n",
        "    labels = batch[3].to(device)\n",
        "\n",
        "    logits = model(input_ids, attention_mask, token_type_ids)\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    loss = torch.masked_select(loss, labels > -1.0).mean()\n",
        "    train_loss.append(loss.item() * input_ids.size(0))\n",
        "\n",
        "    loss.backward()\n",
        "     # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    # it's also improve f1 accuracy slightly\n",
        "    nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "  \n",
        "  return sum(train_loss) / len(train_loss)"
      ],
      "metadata": {
        "id": "Hnqtu5IOtSRz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}